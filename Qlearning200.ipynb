{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training phase...\n",
      "Episode 1/100 - Portfolio Value: 11477.13, Avg Reward: -1342.3074\n",
      "Episode 2/100 - Portfolio Value: 8050.27, Avg Reward: -1.2908\n",
      "Episode 3/100 - Portfolio Value: 10732.82, Avg Reward: -1.3523\n",
      "Episode 4/100 - Portfolio Value: 8562.96, Avg Reward: -1.6683\n",
      "Episode 5/100 - Portfolio Value: 8164.82, Avg Reward: -1.6391\n",
      "Episode 6/100 - Portfolio Value: 12853.12, Avg Reward: -1.4402\n",
      "Episode 7/100 - Portfolio Value: 10044.91, Avg Reward: -1.3134\n",
      "Episode 8/100 - Portfolio Value: 13961.31, Avg Reward: -1.4475\n",
      "Episode 9/100 - Portfolio Value: 23523.83, Avg Reward: -1.2750\n",
      "Episode 10/100 - Portfolio Value: 23207.08, Avg Reward: -1.2296\n",
      "Episode 11/100 - Portfolio Value: 22462.99, Avg Reward: -1.2518\n",
      "Episode 12/100 - Portfolio Value: 19746.82, Avg Reward: -1.4799\n",
      "Episode 13/100 - Portfolio Value: 17178.98, Avg Reward: -1.7937\n",
      "Episode 14/100 - Portfolio Value: 19223.99, Avg Reward: -1.4225\n",
      "Episode 15/100 - Portfolio Value: 14484.05, Avg Reward: -1.7311\n",
      "Episode 16/100 - Portfolio Value: 16041.49, Avg Reward: -1.2129\n",
      "Episode 17/100 - Portfolio Value: 20247.49, Avg Reward: -1.4313\n",
      "Episode 18/100 - Portfolio Value: 26043.52, Avg Reward: -1341.9719\n",
      "Episode 19/100 - Portfolio Value: 23307.73, Avg Reward: -1.6739\n",
      "Episode 20/100 - Portfolio Value: 21604.44, Avg Reward: -1342.7700\n",
      "Episode 21/100 - Portfolio Value: 25206.84, Avg Reward: -1.6009\n",
      "Episode 22/100 - Portfolio Value: 39270.78, Avg Reward: -1.1528\n",
      "Episode 23/100 - Portfolio Value: 29846.38, Avg Reward: -1.0056\n",
      "Episode 24/100 - Portfolio Value: 22456.39, Avg Reward: -0.9864\n",
      "Episode 25/100 - Portfolio Value: 17731.90, Avg Reward: -1.4886\n",
      "Episode 26/100 - Portfolio Value: 20679.34, Avg Reward: -1.1432\n",
      "Episode 27/100 - Portfolio Value: 21696.76, Avg Reward: -1.6573\n",
      "Episode 28/100 - Portfolio Value: 20314.32, Avg Reward: -1.0198\n",
      "Episode 29/100 - Portfolio Value: 19517.99, Avg Reward: -1.1958\n",
      "Episode 30/100 - Portfolio Value: 27996.90, Avg Reward: -0.9226\n",
      "Episode 31/100 - Portfolio Value: 21897.15, Avg Reward: -5366.3902\n",
      "Episode 32/100 - Portfolio Value: 22439.09, Avg Reward: -21452.5201\n",
      "Episode 33/100 - Portfolio Value: 22191.40, Avg Reward: -49613.8366\n",
      "Episode 34/100 - Portfolio Value: 20540.52, Avg Reward: -26815.4568\n",
      "Episode 35/100 - Portfolio Value: 24441.73, Avg Reward: -21453.1151\n",
      "Episode 36/100 - Portfolio Value: 29970.31, Avg Reward: -28156.7116\n",
      "Episode 37/100 - Portfolio Value: 26005.89, Avg Reward: -22798.3047\n",
      "Episode 38/100 - Portfolio Value: 29741.94, Avg Reward: -63015.4094\n",
      "Episode 39/100 - Portfolio Value: 39238.53, Avg Reward: -41562.2859\n",
      "Episode 40/100 - Portfolio Value: 30925.57, Avg Reward: -12068.7027\n",
      "Episode 41/100 - Portfolio Value: 37722.11, Avg Reward: -17432.5609\n",
      "Episode 42/100 - Portfolio Value: 48871.25, Avg Reward: -12068.4547\n",
      "Episode 43/100 - Portfolio Value: 62470.84, Avg Reward: -5365.0873\n",
      "Episode 44/100 - Portfolio Value: 37788.87, Avg Reward: -49601.4769\n",
      "Episode 45/100 - Portfolio Value: 45151.98, Avg Reward: -10727.0330\n",
      "Episode 46/100 - Portfolio Value: 34683.93, Avg Reward: -32181.7925\n",
      "Episode 47/100 - Portfolio Value: 36833.54, Avg Reward: -16093.7415\n",
      "Episode 48/100 - Portfolio Value: 32035.13, Avg Reward: -20113.0639\n",
      "Episode 49/100 - Portfolio Value: 36612.53, Avg Reward: -13408.8478\n",
      "Episode 50/100 - Portfolio Value: 30583.78, Avg Reward: -24134.4144\n",
      "Episode 51/100 - Portfolio Value: 37224.82, Avg Reward: -42899.1021\n",
      "Episode 52/100 - Portfolio Value: 58791.81, Avg Reward: -17430.4752\n",
      "Episode 53/100 - Portfolio Value: 61057.70, Avg Reward: -9389.6047\n",
      "Episode 54/100 - Portfolio Value: 53201.89, Avg Reward: -9389.9808\n",
      "Episode 55/100 - Portfolio Value: 70477.42, Avg Reward: -34858.3172\n",
      "Episode 56/100 - Portfolio Value: 78052.41, Avg Reward: -18774.4112\n",
      "Episode 57/100 - Portfolio Value: 86102.72, Avg Reward: -10726.4520\n",
      "Episode 58/100 - Portfolio Value: 68217.42, Avg Reward: -12069.7642\n",
      "Episode 59/100 - Portfolio Value: 66250.22, Avg Reward: -25476.0963\n",
      "Episode 60/100 - Portfolio Value: 126463.76, Avg Reward: -25475.8009\n",
      "Episode 61/100 - Portfolio Value: 108803.07, Avg Reward: -8051.4426\n",
      "Episode 62/100 - Portfolio Value: 130136.54, Avg Reward: -21456.1814\n",
      "Episode 63/100 - Portfolio Value: 139183.02, Avg Reward: -13413.3144\n",
      "Episode 64/100 - Portfolio Value: 170340.79, Avg Reward: -9388.5728\n",
      "Episode 65/100 - Portfolio Value: 388119.17, Avg Reward: -34855.9187\n",
      "Episode 66/100 - Portfolio Value: 385769.64, Avg Reward: -49606.0160\n",
      "Episode 67/100 - Portfolio Value: 391148.95, Avg Reward: -24134.5610\n",
      "Episode 68/100 - Portfolio Value: 503626.36, Avg Reward: -20156.4721\n",
      "Episode 69/100 - Portfolio Value: 516963.48, Avg Reward: -17429.7641\n",
      "Episode 70/100 - Portfolio Value: 551593.69, Avg Reward: -1342.0298\n",
      "Episode 71/100 - Portfolio Value: 516709.58, Avg Reward: -2682.8765\n",
      "Episode 72/100 - Portfolio Value: 996281.78, Avg Reward: -6704.1119\n",
      "Episode 73/100 - Portfolio Value: 946245.54, Avg Reward: -9384.8470\n",
      "Episode 74/100 - Portfolio Value: 1026700.20, Avg Reward: -4023.0578\n",
      "Episode 75/100 - Portfolio Value: 1743307.19, Avg Reward: -5.1763\n",
      "Episode 76/100 - Portfolio Value: 1772766.10, Avg Reward: -2.8920\n",
      "Episode 77/100 - Portfolio Value: 1607782.34, Avg Reward: -4022.8096\n",
      "Episode 78/100 - Portfolio Value: 1180211.43, Avg Reward: -3.5350\n",
      "Episode 79/100 - Portfolio Value: 1031262.44, Avg Reward: -1343.4347\n",
      "Episode 80/100 - Portfolio Value: 1093702.23, Avg Reward: -1.4412\n",
      "Episode 81/100 - Portfolio Value: 765376.77, Avg Reward: -2683.2682\n",
      "Episode 82/100 - Portfolio Value: 1001640.18, Avg Reward: -16087.3992\n",
      "Episode 83/100 - Portfolio Value: 1084078.76, Avg Reward: -16088.0574\n",
      "Episode 84/100 - Portfolio Value: 690382.66, Avg Reward: -4023.4397\n",
      "Episode 85/100 - Portfolio Value: 647623.64, Avg Reward: -4024.3015\n",
      "Episode 86/100 - Portfolio Value: 763375.19, Avg Reward: -46925.0771\n",
      "Episode 87/100 - Portfolio Value: 1019105.26, Avg Reward: -14749.1360\n",
      "Episode 88/100 - Portfolio Value: 1308554.00, Avg Reward: -8046.5021\n",
      "Episode 89/100 - Portfolio Value: 1470478.70, Avg Reward: -8047.1680\n",
      "Episode 90/100 - Portfolio Value: 977749.71, Avg Reward: -10728.1813\n",
      "Episode 91/100 - Portfolio Value: 704040.72, Avg Reward: -6707.5722\n",
      "Episode 92/100 - Portfolio Value: 545006.54, Avg Reward: -18770.9888\n",
      "Episode 93/100 - Portfolio Value: 671476.69, Avg Reward: -4023.9508\n",
      "Episode 94/100 - Portfolio Value: 744296.10, Avg Reward: -4024.0372\n",
      "Episode 95/100 - Portfolio Value: 1146152.93, Avg Reward: -33517.8059\n",
      "Episode 96/100 - Portfolio Value: 1699545.98, Avg Reward: -2681.9087\n",
      "Episode 97/100 - Portfolio Value: 1531580.76, Avg Reward: -5363.6939\n",
      "Episode 98/100 - Portfolio Value: 2058871.25, Avg Reward: -1.2069\n",
      "Episode 99/100 - Portfolio Value: 1786943.85, Avg Reward: -6704.5866\n",
      "Episode 100/100 - Portfolio Value: 1081325.08, Avg Reward: -1342.3670\n",
      "Agent saved successfully to saved_models/QLaapl200_agent.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting trading phase on test data...\n",
      "Day 2023-01-23 00:00:00: Bought 70.0 shares at 141.11\n",
      "Day 2023-01-31 00:00:00: Sold 35.0 shares at 144.29\n",
      "Day 2023-02-03 00:00:00: Sold 33.0 shares at 154.50\n",
      "Day 2023-02-06 00:00:00: Bought 6.0 shares at 151.73\n",
      "Day 2023-02-07 00:00:00: Sold 8.0 shares at 154.65\n",
      "Day 2023-02-10 00:00:00: Bought 70.0 shares at 151.01\n",
      "Day 2023-02-17 00:00:00: Sold 7.0 shares at 152.55\n",
      "Day 2023-02-21 00:00:00: Sold 36.0 shares at 148.48\n",
      "Day 2023-02-24 00:00:00: Sold 27.0 shares at 146.71\n",
      "Day 2023-03-09 00:00:00: Bought 6.0 shares at 150.59\n",
      "Day 2023-03-13 00:00:00: Sold 6.0 shares at 150.47\n",
      "Day 2023-03-28 00:00:00: Bought 32.0 shares at 157.65\n",
      "Day 2023-03-29 00:00:00: Sold 32.0 shares at 160.77\n",
      "Day 2023-03-30 00:00:00: Bought 6.0 shares at 162.36\n",
      "Day 2023-04-03 00:00:00: Bought 6.0 shares at 166.17\n",
      "Day 2023-04-06 00:00:00: Bought 6.0 shares at 164.66\n",
      "Day 2023-04-10 00:00:00: Bought 32.0 shares at 162.03\n",
      "Day 2023-04-12 00:00:00: Bought 6.0 shares at 160.10\n",
      "Day 2023-04-17 00:00:00: Bought 8.0 shares at 165.23\n",
      "Day 2023-04-18 00:00:00: Sold 63.0 shares at 166.47\n",
      "Day 2023-04-21 00:00:00: Bought 64.0 shares at 165.02\n",
      "Day 2023-04-24 00:00:00: Sold 32.0 shares at 165.33\n",
      "Day 2023-04-27 00:00:00: Sold 6.0 shares at 168.41\n",
      "Day 2023-04-28 00:00:00: Sold 27.0 shares at 169.68\n",
      "Day 2023-05-03 00:00:00: Bought 6.0 shares at 167.45\n",
      "Day 2023-05-04 00:00:00: Bought 59.0 shares at 165.79\n",
      "Day 2023-05-12 00:00:00: Sold 65.0 shares at 172.57\n",
      "Day 2023-05-16 00:00:00: Bought 65.0 shares at 172.07\n",
      "Day 2023-05-30 00:00:00: Sold 32.0 shares at 177.30\n",
      "Day 2023-06-05 00:00:00: Sold 32.0 shares at 179.58\n",
      "Day 2023-06-06 00:00:00: Bought 32.0 shares at 179.21\n",
      "Day 2023-06-08 00:00:00: Sold 32.0 shares at 180.57\n",
      "Day 2023-06-09 00:00:00: Sold 1.0 shares at 180.96\n",
      "Day 2023-06-15 00:00:00: Bought 31.0 shares at 186.01\n",
      "Day 2023-06-26 00:00:00: Bought 32.0 shares at 185.27\n",
      "Day 2023-06-30 00:00:00: Sold 61.0 shares at 193.97\n",
      "Day 2023-07-10 00:00:00: Sold 2.0 shares at 188.61\n",
      "Day 2023-07-17 00:00:00: Bought 63.0 shares at 193.99\n",
      "Day 2023-07-19 00:00:00: Sold 6.0 shares at 195.10\n",
      "Day 2023-07-20 00:00:00: Bought 6.0 shares at 193.13\n",
      "Day 2023-08-09 00:00:00: Sold 32.0 shares at 178.19\n",
      "Day 2023-08-10 00:00:00: Sold 31.0 shares at 177.97\n",
      "Day 2023-08-11 00:00:00: Bought 63.0 shares at 177.79\n",
      "Day 2023-08-14 00:00:00: Sold 6.0 shares at 179.46\n",
      "Day 2023-08-17 00:00:00: Bought 6.0 shares at 174.00\n",
      "Day 2023-08-23 00:00:00: Sold 62.0 shares at 181.12\n",
      "Day 2023-08-25 00:00:00: Bought 32.0 shares at 178.61\n",
      "Day 2023-08-29 00:00:00: Bought 30.0 shares at 184.12\n",
      "Day 2023-08-30 00:00:00: Sold 62.0 shares at 187.65\n",
      "Day 2023-09-01 00:00:00: Bought 62.0 shares at 189.46\n",
      "Day 2023-09-07 00:00:00: Sold 63.0 shares at 177.56\n",
      "Day 2023-09-19 00:00:00: Bought 62.0 shares at 179.07\n",
      "Day 2023-09-25 00:00:00: Sold 6.0 shares at 176.08\n",
      "Day 2023-09-27 00:00:00: Sold 31.0 shares at 170.43\n",
      "Day 2023-10-03 00:00:00: Bought 31.0 shares at 172.40\n",
      "Day 2023-10-05 00:00:00: Sold 56.0 shares at 174.91\n",
      "Day 2023-10-09 00:00:00: Bought 30.0 shares at 178.99\n",
      "Day 2023-10-11 00:00:00: Sold 6.0 shares at 179.80\n",
      "Day 2023-10-12 00:00:00: Sold 24.0 shares at 180.71\n",
      "Day 2023-10-26 00:00:00: Bought 32.0 shares at 166.89\n",
      "Day 2023-11-01 00:00:00: Sold 6.0 shares at 173.97\n",
      "Day 2023-11-02 00:00:00: Bought 37.0 shares at 177.57\n",
      "Day 2023-11-08 00:00:00: Sold 31.0 shares at 182.89\n",
      "Day 2023-11-09 00:00:00: Bought 6.0 shares at 182.41\n",
      "Day 2023-11-16 00:00:00: Sold 31.0 shares at 189.71\n",
      "Day 2023-11-20 00:00:00: Sold 6.0 shares at 191.45\n",
      "Day 2023-11-22 00:00:00: Sold 1.0 shares at 191.31\n",
      "Day 2023-11-24 00:00:00: Bought 31.0 shares at 189.97\n",
      "Day 2023-11-28 00:00:00: Bought 31.0 shares at 190.40\n",
      "Day 2023-11-30 00:00:00: Sold 6.0 shares at 189.95\n",
      "Day 2023-12-08 00:00:00: Sold 6.0 shares at 195.71\n",
      "Day 2023-12-11 00:00:00: Bought 12.0 shares at 193.18\n",
      "Day 2023-12-14 00:00:00: Sold 6.0 shares at 198.11\n",
      "Day 2023-12-18 00:00:00: Bought 6.0 shares at 195.89\n",
      "Day 2023-12-20 00:00:00: Sold 31.0 shares at 194.83\n",
      "Day 2023-12-27 00:00:00: Sold 6.0 shares at 193.15\n",
      "\n",
      "Evaluation Metrics on Test Data:\n",
      "Total Return (%): 20.87\n",
      "Sharpe Ratio: -1.03\n",
      "Max Drawdown (%): 14.15\n",
      "Profit-Loss Ratio: 0.27\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# Constants\n",
    "EPSILON = 1e-8\n",
    "action_space = np.array([-1.0, -0.5, -0.1, 0, 0.1, 0.5, 1.0])\n",
    "temperature = 1.0  # Softmax temperature\n",
    "WINDOW_SIZE = 10\n",
    "RISK_FREE_RATE = 0.01  # Assuming a 1% risk-free rate\n",
    "\n",
    "def choose_action(action, portfolio_value, cash, shares, current_price):\n",
    "    \"\"\"\n",
    "    Determine the number of shares to trade based on the action\n",
    "    Returns positive number for buying, negative for selling\n",
    "    \"\"\"\n",
    "    max_possible_shares = cash / current_price\n",
    "    if action > 0:  # Buy\n",
    "        shares_to_trade = min(max_possible_shares, portfolio_value * abs(action) / current_price)\n",
    "        return np.floor(shares_to_trade)\n",
    "    elif action < 0:  # Sell\n",
    "        shares_to_trade = min(shares, portfolio_value * abs(action) / current_price)\n",
    "        return -np.floor(shares_to_trade)\n",
    "    return 0\n",
    "\n",
    "class LSTMTrader(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMTrader, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        final_output = lstm_out[:, -1, :]\n",
    "        return self.fc(final_output)\n",
    "\n",
    "def load_data(ticker=\"AAPL\", start_date=\"2020-01-01\", end_date=\"2023-01-01\"):\n",
    "    df = yf.download(ticker, start=start_date, end=end_date)\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def softmax(q_values, temperature):\n",
    "    if torch.any(torch.isnan(q_values)):\n",
    "        print(\"NaN found in q_values!\")\n",
    "        return torch.zeros_like(q_values)  # Return zero if NaNs are found\n",
    "    scaled_qs = q_values / temperature\n",
    "    exp_qs = torch.exp(scaled_qs - torch.max(scaled_qs))  # Subtract max for numerical stability\n",
    "    probs = exp_qs / (torch.sum(exp_qs) + EPSILON)  # Avoid division by zero\n",
    "    return torch.clamp(probs, min=0.0, max=1.0)  # Ensure valid probabilities\n",
    "\n",
    "def choose_action_softmax(q_values):\n",
    "    action_probs = softmax(q_values, temperature).detach().numpy()\n",
    "    if np.any(np.isnan(action_probs)):\n",
    "        print(f\"NaN found in action probabilities: {action_probs}\")\n",
    "        action_probs = np.ones_like(action_probs) / len(action_probs)  # Fallback to uniform distribution\n",
    "    return np.random.choice(len(action_space), p=action_probs)\n",
    "\n",
    "def reward_function(new_portfolio_value, prev_portfolio_value, returns, window=WINDOW_SIZE):\n",
    "    reward = (new_portfolio_value - prev_portfolio_value) / (prev_portfolio_value + EPSILON)\n",
    "    if len(returns) >= window:\n",
    "        returns_array = list(returns)[-window:]\n",
    "        sharpe_ratio = calculate_sharpe_ratio(returns_array)\n",
    "        reward += sharpe_ratio\n",
    "    return reward\n",
    "\n",
    "def calculate_sharpe_ratio(returns):\n",
    "    average_return = np.mean(returns)  # Average returns\n",
    "    excess_return = average_return - RISK_FREE_RATE  # Average return minus risk-free rate\n",
    "    std_dev = np.std(returns) + EPSILON  # Standard deviation of returns\n",
    "    return excess_return / std_dev  # Sharpe ratio calculation\n",
    "\n",
    "\n",
    "def train_rl_agent(model, optimizer, loss_fn, df, discount_factor, start_date, end_date, initial_portfolio_value=10000):\n",
    "    returns = deque(maxlen=WINDOW_SIZE)\n",
    "    portfolio_value = initial_portfolio_value\n",
    "    cash = initial_portfolio_value\n",
    "    shares = 0\n",
    "    \n",
    "    train_data = df.loc[start_date:end_date]\n",
    "    \n",
    "    for episode in range(100):\n",
    "        state = train_data.iloc[:WINDOW_SIZE].values\n",
    "        episode_rewards = []\n",
    "        \n",
    "        for t in range(WINDOW_SIZE, len(train_data)):\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            q_values = model(state_tensor)\n",
    "            action_idx = choose_action_softmax(q_values[0])\n",
    "            action = action_space[action_idx]\n",
    "            \n",
    "            current_price = train_data.iloc[t]['Close']\n",
    "            shares_to_trade = choose_action(action, portfolio_value, cash, shares, current_price)\n",
    "            \n",
    "            # Update portfolio\n",
    "            if shares_to_trade > 0:  # Buying\n",
    "                cash -= shares_to_trade * current_price\n",
    "                shares += shares_to_trade\n",
    "            else:  # Selling\n",
    "                cash -= shares_to_trade * current_price  # Note: shares_to_trade is negative\n",
    "                shares += shares_to_trade\n",
    "            \n",
    "            new_portfolio_value = cash + shares * current_price\n",
    "            new_portfolio_value = np.clip(new_portfolio_value, 1, 1e10)\n",
    "\n",
    "            reward = reward_function(new_portfolio_value, portfolio_value, returns)\n",
    "            episode_rewards.append(reward)\n",
    "            returns.append(new_portfolio_value / portfolio_value - 1)  # Store relative return\n",
    "\n",
    "            next_state = train_data.iloc[t - WINDOW_SIZE + 1:t + 1].values\n",
    "            next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "            next_q_values = model(next_state_tensor)\n",
    "            target_q_values = reward + discount_factor * torch.max(next_q_values)\n",
    "            \n",
    "            current_q = q_values[0, action_idx]\n",
    "            loss = loss_fn(current_q, target_q_values)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "\n",
    "            state = next_state\n",
    "            portfolio_value = new_portfolio_value\n",
    "\n",
    "        avg_reward = np.mean(episode_rewards)\n",
    "        print(f'Episode {episode + 1}/100 - Portfolio Value: {portfolio_value:.2f}, Avg Reward: {avg_reward:.4f}')\n",
    "\n",
    "def trade(model, df, start_date, end_date, initial_portfolio_value=10000):\n",
    "    \"\"\"\n",
    "    Performs trading on the specified data range and records trades and portfolio values.\n",
    "    \"\"\"\n",
    "    portfolio_value = initial_portfolio_value\n",
    "    cash = initial_portfolio_value\n",
    "    shares = 0\n",
    "    trades = []\n",
    "    daily_values = []\n",
    "    trading_temperature = 0.5  # Lower temperature for more deterministic trading\n",
    "\n",
    "    trade_data = df.loc[start_date:end_date]\n",
    "    if len(trade_data) <= WINDOW_SIZE:\n",
    "        print(f\"Error: Not enough data for trading. Available data points: {len(trade_data)}\")\n",
    "        return trades, daily_values\n",
    "\n",
    "    state = trade_data.iloc[:WINDOW_SIZE].values\n",
    "\n",
    "    for t in range(WINDOW_SIZE, len(trade_data)):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        q_values = model(state_tensor)\n",
    "        \n",
    "        action_probs = softmax(q_values[0], trading_temperature).detach().numpy()\n",
    "        action_idx = np.random.choice(len(action_space), p=action_probs)\n",
    "        action = action_space[action_idx]\n",
    "        \n",
    "        current_price = trade_data.iloc[t]['Close']\n",
    "        shares_to_trade = choose_action(action, portfolio_value, cash, shares, current_price)\n",
    "        \n",
    "        if shares_to_trade > 0 and cash >= shares_to_trade * current_price:  # Buy\n",
    "            cash -= shares_to_trade * current_price\n",
    "            shares += shares_to_trade\n",
    "            trades.append(('buy', shares_to_trade, current_price))\n",
    "            print(f\"Day {trade_data.index[t]}: Bought {shares_to_trade} shares at {current_price:.2f}\")\n",
    "        elif shares_to_trade < 0 and shares >= abs(shares_to_trade):  # Sell\n",
    "            cash -= shares_to_trade * current_price  # Remember shares_to_trade is negative\n",
    "            shares += shares_to_trade\n",
    "            trades.append(('sell', abs(shares_to_trade), current_price))\n",
    "            print(f\"Day {trade_data.index[t]}: Sold {abs(shares_to_trade)} shares at {current_price:.2f}\")\n",
    "        \n",
    "        portfolio_value = cash + shares * current_price\n",
    "        daily_values.append((trade_data.index[t], portfolio_value))\n",
    "        state = trade_data.iloc[t - WINDOW_SIZE + 1:t + 1].values\n",
    "\n",
    "    return trades, daily_values\n",
    "\n",
    "def evaluate_performance(daily_values, initial_portfolio_value):\n",
    "    \"\"\"\n",
    "    Evaluates performance metrics based on daily portfolio values.\n",
    "    \"\"\"\n",
    "    start_value = daily_values[0][1]\n",
    "    end_value = daily_values[-1][1]\n",
    "    total_return = (end_value - start_value) / start_value * 100\n",
    "\n",
    "    # Calculate daily returns and Sharpe ratio\n",
    "    returns = [v[1] / daily_values[i-1][1] - 1 for i, v in enumerate(daily_values) if i > 0]\n",
    "    sharpe_ratio = calculate_sharpe_ratio(returns)\n",
    "\n",
    "    # Calculate max drawdown as a percentage\n",
    "    peak = daily_values[0][1]\n",
    "    max_drawdown = 0\n",
    "    for _, value in daily_values:\n",
    "        peak = max(peak, value)\n",
    "        drawdown = (peak - value) / peak\n",
    "        max_drawdown = max(max_drawdown, drawdown)\n",
    "    max_drawdown_percent = max_drawdown * 100\n",
    "\n",
    "    # Calculate Profit-Loss Ratio (using total PnL and total trades)\n",
    "    total_pnl = end_value - initial_portfolio_value\n",
    "    total_trades = len([t for t in trades if t[0] == 'buy']) + len([t for t in trades if t[0] == 'sell'])\n",
    "    profit_loss_ratio = total_pnl / (total_trades * initial_portfolio_value / 100) if total_trades > 0 else 0\n",
    "\n",
    "    return total_return, sharpe_ratio, max_drawdown_percent, profit_loss_ratio\n",
    "\n",
    "# Function to save the trained model and optimizer\n",
    "def save_agent(model, optimizer, file_path=\"agent.pth\"):\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, file_path)\n",
    "    print(f\"Agent saved successfully to {file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Load training data and define model, optimizer, and loss function\n",
    "    df = load_data()\n",
    "    model = LSTMTrader(input_size=df.shape[1], hidden_size=64, output_size=len(action_space))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting training phase...\")\n",
    "    train_rl_agent(model, optimizer, loss_fn, df, discount_factor=0.99, \n",
    "                   start_date='2020-01-01', end_date='2023-01-01')\n",
    "    \n",
    "    # After training, save the agent\n",
    "    save_agent(model, optimizer, file_path=\"saved_models/QLaapl200_agent.pth\")\n",
    "\n",
    "    \n",
    "    # Load test data\n",
    "    test_data = load_data(start_date='2023-01-01', end_date='2024-01-01')\n",
    "    \n",
    "    # Test the model and calculate metrics on test data\n",
    "    print(\"\\nStarting trading phase on test data...\")\n",
    "    trades, daily_values = trade(model, test_data, start_date='2023-01-01', end_date='2024-01-01')\n",
    "    \n",
    "    if not daily_values:\n",
    "        print(\"Error: No trades were executed during the trading period.\")\n",
    "    else:\n",
    "        total_return, sharpe_ratio, max_drawdown, profit_loss_ratio = evaluate_performance(daily_values, 10000)\n",
    "        \n",
    "        print(\"\\nEvaluation Metrics on Test Data:\")\n",
    "        print(f\"Total Return (%): {total_return:.2f}\")\n",
    "        print(f\"Sharpe Ratio: {sharpe_ratio:.2f}\")\n",
    "        print(f\"Max Drawdown (%): {max_drawdown:.2f}\")\n",
    "        print(f\"Profit-Loss Ratio: {profit_loss_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Softmax #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for AAPL...\n",
      "Episode 1, Total Reward: 0.16, Epsilon: 0.08\n",
      "Episode 2, Total Reward: -0.18, Epsilon: 0.06\n",
      "Episode 3, Total Reward: -0.03, Epsilon: 0.05\n",
      "Episode 4, Total Reward: 0.20, Epsilon: 0.04\n",
      "Episode 5, Total Reward: -0.39, Epsilon: 0.03\n",
      "Episode 6, Total Reward: -0.01, Epsilon: 0.03\n",
      "Episode 7, Total Reward: 0.23, Epsilon: 0.02\n",
      "Episode 8, Total Reward: -0.25, Epsilon: 0.02\n",
      "Episode 9, Total Reward: -0.10, Epsilon: 0.01\n",
      "Episode 10, Total Reward: 0.19, Epsilon: 0.01\n",
      "Episode 11, Total Reward: 0.52, Epsilon: 0.01\n",
      "Episode 12, Total Reward: 0.13, Epsilon: 0.01\n",
      "Episode 13, Total Reward: -0.06, Epsilon: 0.01\n",
      "Episode 14, Total Reward: 0.13, Epsilon: 0.01\n",
      "Episode 15, Total Reward: -0.13, Epsilon: 0.01\n",
      "Episode 16, Total Reward: -0.03, Epsilon: 0.01\n",
      "Episode 17, Total Reward: 0.27, Epsilon: 0.01\n",
      "Episode 18, Total Reward: -0.04, Epsilon: 0.01\n",
      "Episode 19, Total Reward: 0.58, Epsilon: 0.01\n",
      "Episode 20, Total Reward: 0.01, Epsilon: 0.01\n",
      "Episode 21, Total Reward: 0.03, Epsilon: 0.01\n",
      "Episode 22, Total Reward: 0.47, Epsilon: 0.01\n",
      "Episode 23, Total Reward: 0.29, Epsilon: 0.01\n",
      "Episode 24, Total Reward: -0.15, Epsilon: 0.01\n",
      "Episode 25, Total Reward: 0.31, Epsilon: 0.01\n",
      "Episode 26, Total Reward: 0.10, Epsilon: 0.01\n",
      "Episode 27, Total Reward: 0.17, Epsilon: 0.01\n",
      "Episode 28, Total Reward: -0.05, Epsilon: 0.01\n",
      "Episode 29, Total Reward: 0.50, Epsilon: 0.01\n",
      "Episode 30, Total Reward: 0.53, Epsilon: 0.01\n",
      "Episode 31, Total Reward: 0.32, Epsilon: 0.01\n",
      "Episode 32, Total Reward: 0.23, Epsilon: 0.01\n",
      "Episode 33, Total Reward: -0.25, Epsilon: 0.01\n",
      "Episode 34, Total Reward: 0.07, Epsilon: 0.01\n",
      "Episode 35, Total Reward: 0.27, Epsilon: 0.01\n",
      "Episode 36, Total Reward: 0.66, Epsilon: 0.01\n",
      "Episode 37, Total Reward: -0.11, Epsilon: 0.01\n",
      "Episode 38, Total Reward: -0.25, Epsilon: 0.01\n",
      "Episode 39, Total Reward: 0.41, Epsilon: 0.01\n",
      "Episode 40, Total Reward: -0.47, Epsilon: 0.01\n",
      "Episode 41, Total Reward: -0.28, Epsilon: 0.01\n",
      "Episode 42, Total Reward: -0.19, Epsilon: 0.01\n",
      "Episode 43, Total Reward: 0.01, Epsilon: 0.01\n",
      "Episode 44, Total Reward: -0.09, Epsilon: 0.01\n",
      "Episode 45, Total Reward: 0.14, Epsilon: 0.01\n",
      "Episode 46, Total Reward: 0.13, Epsilon: 0.01\n",
      "Episode 47, Total Reward: 0.00, Epsilon: 0.01\n",
      "Episode 48, Total Reward: -0.12, Epsilon: 0.01\n",
      "Episode 49, Total Reward: -0.15, Epsilon: 0.01\n",
      "Episode 50, Total Reward: 0.38, Epsilon: 0.01\n",
      "Episode 51, Total Reward: 0.31, Epsilon: 0.01\n",
      "Episode 52, Total Reward: -0.14, Epsilon: 0.01\n",
      "Episode 53, Total Reward: -0.21, Epsilon: 0.01\n",
      "Episode 54, Total Reward: -0.24, Epsilon: 0.01\n",
      "Episode 55, Total Reward: 0.53, Epsilon: 0.01\n",
      "Episode 56, Total Reward: 0.60, Epsilon: 0.01\n",
      "Episode 57, Total Reward: 0.49, Epsilon: 0.01\n",
      "Episode 58, Total Reward: 0.32, Epsilon: 0.01\n",
      "Episode 59, Total Reward: 0.03, Epsilon: 0.01\n",
      "Episode 60, Total Reward: 0.12, Epsilon: 0.01\n",
      "Episode 61, Total Reward: -0.10, Epsilon: 0.01\n",
      "Episode 62, Total Reward: 0.31, Epsilon: 0.01\n",
      "Episode 63, Total Reward: -0.03, Epsilon: 0.01\n",
      "Episode 64, Total Reward: 0.57, Epsilon: 0.01\n",
      "Episode 65, Total Reward: 0.50, Epsilon: 0.01\n",
      "Episode 66, Total Reward: -0.05, Epsilon: 0.01\n",
      "Episode 67, Total Reward: -0.10, Epsilon: 0.01\n",
      "Episode 68, Total Reward: 0.22, Epsilon: 0.01\n",
      "Episode 69, Total Reward: 0.18, Epsilon: 0.01\n",
      "Episode 70, Total Reward: -0.10, Epsilon: 0.01\n",
      "Episode 71, Total Reward: -0.36, Epsilon: 0.01\n",
      "Episode 72, Total Reward: 0.52, Epsilon: 0.01\n",
      "Episode 73, Total Reward: 0.23, Epsilon: 0.01\n",
      "Episode 74, Total Reward: 0.17, Epsilon: 0.01\n",
      "Episode 75, Total Reward: 0.23, Epsilon: 0.01\n",
      "Episode 76, Total Reward: 0.06, Epsilon: 0.01\n",
      "Episode 77, Total Reward: -0.23, Epsilon: 0.01\n",
      "Episode 78, Total Reward: -0.18, Epsilon: 0.01\n",
      "Episode 79, Total Reward: -0.15, Epsilon: 0.01\n",
      "Episode 80, Total Reward: -0.02, Epsilon: 0.01\n",
      "Episode 81, Total Reward: -0.00, Epsilon: 0.01\n",
      "Episode 82, Total Reward: -0.05, Epsilon: 0.01\n",
      "Episode 83, Total Reward: -0.05, Epsilon: 0.01\n",
      "Episode 84, Total Reward: 0.12, Epsilon: 0.01\n",
      "Episode 85, Total Reward: 0.27, Epsilon: 0.01\n",
      "Episode 86, Total Reward: 0.23, Epsilon: 0.01\n",
      "Episode 87, Total Reward: 0.17, Epsilon: 0.01\n",
      "Episode 88, Total Reward: 0.24, Epsilon: 0.01\n",
      "Episode 89, Total Reward: 0.25, Epsilon: 0.01\n",
      "Episode 90, Total Reward: 0.23, Epsilon: 0.01\n",
      "Episode 91, Total Reward: 0.23, Epsilon: 0.01\n",
      "Episode 92, Total Reward: 0.26, Epsilon: 0.01\n",
      "Episode 93, Total Reward: 0.31, Epsilon: 0.01\n",
      "Episode 94, Total Reward: 0.07, Epsilon: 0.01\n",
      "Episode 95, Total Reward: -0.22, Epsilon: 0.01\n",
      "Episode 96, Total Reward: 0.38, Epsilon: 0.01\n",
      "Episode 97, Total Reward: -0.01, Epsilon: 0.01\n",
      "Episode 98, Total Reward: 0.02, Epsilon: 0.01\n",
      "Episode 99, Total Reward: 0.22, Epsilon: 0.01\n",
      "Episode 100, Total Reward: -0.06, Epsilon: 0.01\n",
      "Model, optimizer, and parameters for AAPL saved to AAPL_EnsembleModel.pth\n",
      "\n",
      "Training model for IBM...\n",
      "Episode 1, Total Reward: -0.14, Epsilon: 0.08\n",
      "Episode 2, Total Reward: 0.08, Epsilon: 0.06\n",
      "Episode 3, Total Reward: 0.31, Epsilon: 0.05\n",
      "Episode 4, Total Reward: 0.26, Epsilon: 0.04\n",
      "Episode 5, Total Reward: -0.26, Epsilon: 0.03\n",
      "Episode 6, Total Reward: 0.00, Epsilon: 0.03\n",
      "Episode 7, Total Reward: 0.12, Epsilon: 0.02\n",
      "Episode 8, Total Reward: 0.46, Epsilon: 0.02\n",
      "Episode 9, Total Reward: -0.09, Epsilon: 0.01\n",
      "Episode 10, Total Reward: 0.04, Epsilon: 0.01\n",
      "Episode 11, Total Reward: 0.10, Epsilon: 0.01\n",
      "Episode 12, Total Reward: 0.17, Epsilon: 0.01\n",
      "Episode 13, Total Reward: -0.20, Epsilon: 0.01\n",
      "Episode 14, Total Reward: 0.45, Epsilon: 0.01\n",
      "Episode 15, Total Reward: 0.19, Epsilon: 0.01\n",
      "Episode 16, Total Reward: -0.04, Epsilon: 0.01\n",
      "Episode 17, Total Reward: -0.07, Epsilon: 0.01\n",
      "Episode 18, Total Reward: 0.34, Epsilon: 0.01\n",
      "Episode 19, Total Reward: -0.31, Epsilon: 0.01\n",
      "Episode 20, Total Reward: 0.12, Epsilon: 0.01\n",
      "Episode 21, Total Reward: 0.14, Epsilon: 0.01\n",
      "Episode 22, Total Reward: -0.13, Epsilon: 0.01\n",
      "Episode 23, Total Reward: -0.02, Epsilon: 0.01\n",
      "Episode 24, Total Reward: -0.05, Epsilon: 0.01\n",
      "Episode 25, Total Reward: 0.14, Epsilon: 0.01\n",
      "Episode 26, Total Reward: 0.13, Epsilon: 0.01\n",
      "Episode 27, Total Reward: 0.02, Epsilon: 0.01\n",
      "Episode 28, Total Reward: 0.58, Epsilon: 0.01\n",
      "Episode 29, Total Reward: 0.48, Epsilon: 0.01\n",
      "Episode 30, Total Reward: -0.09, Epsilon: 0.01\n",
      "Episode 31, Total Reward: -0.16, Epsilon: 0.01\n",
      "Episode 32, Total Reward: 0.11, Epsilon: 0.01\n",
      "Episode 33, Total Reward: -0.16, Epsilon: 0.01\n",
      "Episode 34, Total Reward: 0.06, Epsilon: 0.01\n",
      "Episode 35, Total Reward: -0.14, Epsilon: 0.01\n",
      "Episode 36, Total Reward: -0.25, Epsilon: 0.01\n",
      "Episode 37, Total Reward: 0.12, Epsilon: 0.01\n",
      "Episode 38, Total Reward: 0.36, Epsilon: 0.01\n",
      "Episode 39, Total Reward: -0.05, Epsilon: 0.01\n",
      "Episode 40, Total Reward: 0.19, Epsilon: 0.01\n",
      "Episode 41, Total Reward: -0.10, Epsilon: 0.01\n",
      "Episode 42, Total Reward: 0.13, Epsilon: 0.01\n",
      "Episode 43, Total Reward: 0.28, Epsilon: 0.01\n",
      "Episode 44, Total Reward: 0.36, Epsilon: 0.01\n",
      "Episode 45, Total Reward: 0.16, Epsilon: 0.01\n",
      "Episode 46, Total Reward: -0.13, Epsilon: 0.01\n",
      "Episode 47, Total Reward: 0.31, Epsilon: 0.01\n",
      "Episode 48, Total Reward: 0.16, Epsilon: 0.01\n",
      "Episode 49, Total Reward: 0.27, Epsilon: 0.01\n",
      "Episode 50, Total Reward: 0.01, Epsilon: 0.01\n",
      "Episode 51, Total Reward: 0.05, Epsilon: 0.01\n",
      "Episode 52, Total Reward: 0.05, Epsilon: 0.01\n",
      "Episode 53, Total Reward: 0.12, Epsilon: 0.01\n",
      "Episode 54, Total Reward: 0.12, Epsilon: 0.01\n",
      "Episode 55, Total Reward: 0.03, Epsilon: 0.01\n",
      "Episode 56, Total Reward: 0.06, Epsilon: 0.01\n",
      "Episode 57, Total Reward: -0.04, Epsilon: 0.01\n",
      "Episode 58, Total Reward: 0.04, Epsilon: 0.01\n",
      "Episode 59, Total Reward: 0.26, Epsilon: 0.01\n",
      "Episode 60, Total Reward: -0.14, Epsilon: 0.01\n",
      "Episode 61, Total Reward: 0.01, Epsilon: 0.01\n",
      "Episode 62, Total Reward: -0.08, Epsilon: 0.01\n",
      "Episode 63, Total Reward: 0.43, Epsilon: 0.01\n",
      "Episode 64, Total Reward: -0.16, Epsilon: 0.01\n",
      "Episode 65, Total Reward: 0.31, Epsilon: 0.01\n",
      "Episode 66, Total Reward: -0.14, Epsilon: 0.01\n",
      "Episode 67, Total Reward: 0.37, Epsilon: 0.01\n",
      "Episode 68, Total Reward: -0.24, Epsilon: 0.01\n",
      "Episode 69, Total Reward: -0.25, Epsilon: 0.01\n",
      "Episode 70, Total Reward: -0.06, Epsilon: 0.01\n",
      "Episode 71, Total Reward: 0.05, Epsilon: 0.01\n",
      "Episode 72, Total Reward: 0.16, Epsilon: 0.01\n",
      "Episode 73, Total Reward: 0.20, Epsilon: 0.01\n",
      "Episode 74, Total Reward: 0.34, Epsilon: 0.01\n",
      "Episode 75, Total Reward: -0.14, Epsilon: 0.01\n",
      "Episode 76, Total Reward: -0.08, Epsilon: 0.01\n",
      "Episode 77, Total Reward: -0.31, Epsilon: 0.01\n",
      "Episode 78, Total Reward: -0.38, Epsilon: 0.01\n",
      "Episode 79, Total Reward: 0.09, Epsilon: 0.01\n",
      "Episode 80, Total Reward: -0.41, Epsilon: 0.01\n",
      "Episode 81, Total Reward: -0.18, Epsilon: 0.01\n",
      "Episode 82, Total Reward: -0.27, Epsilon: 0.01\n",
      "Episode 83, Total Reward: -0.29, Epsilon: 0.01\n",
      "Episode 84, Total Reward: 0.05, Epsilon: 0.01\n",
      "Episode 85, Total Reward: 0.31, Epsilon: 0.01\n",
      "Episode 86, Total Reward: -0.21, Epsilon: 0.01\n",
      "Episode 87, Total Reward: -0.19, Epsilon: 0.01\n",
      "Episode 88, Total Reward: 0.13, Epsilon: 0.01\n",
      "Episode 89, Total Reward: 0.74, Epsilon: 0.01\n",
      "Episode 90, Total Reward: -0.04, Epsilon: 0.01\n",
      "Episode 91, Total Reward: 0.16, Epsilon: 0.01\n",
      "Episode 92, Total Reward: -0.17, Epsilon: 0.01\n",
      "Episode 93, Total Reward: -0.00, Epsilon: 0.01\n",
      "Episode 94, Total Reward: -0.30, Epsilon: 0.01\n",
      "Episode 95, Total Reward: -0.16, Epsilon: 0.01\n",
      "Episode 96, Total Reward: -0.32, Epsilon: 0.01\n",
      "Episode 97, Total Reward: -0.18, Epsilon: 0.01\n",
      "Episode 98, Total Reward: -0.04, Epsilon: 0.01\n",
      "Episode 99, Total Reward: -0.21, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Total Reward: 0.09, Epsilon: 0.01\n",
      "Model, optimizer, and parameters for IBM saved to IBM_EnsembleModel.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running trading simulation on test period...\n",
      "\n",
      "Testing Results (Evaluation Metrics):\n",
      "==================================================\n",
      "Final Balance: $11582.65\n",
      "Total Returns: 15.83%\n",
      "Win Rate: 0.43\n",
      "Volatility: 0.0089\n",
      "Sharpe Ratio: 1.2291\n",
      "\n",
      "Final Positions:\n",
      "AAPL: 0 shares\n",
      "IBM: 70 shares\n",
      "Cash: $134.15\n",
      "\n",
      "Trading History (first 10 trades in test period):\n",
      "Day 3: IBM - BUY 72 shares at $136.94\n",
      "Day 16: IBM - SELL 72 shares at $130.79\n",
      "Day 21: IBM - BUY 73 shares at $128.93\n",
      "Day 22: IBM - SELL 73 shares at $129.64\n",
      "Day 23: IBM - BUY 73 shares at $130.19\n",
      "Day 25: IBM - SELL 73 shares at $128.05\n",
      "Day 27: IBM - BUY 74 shares at $125.45\n",
      "Day 38: IBM - SELL 74 shares at $129.31\n",
      "Day 41: IBM - BUY 74 shares at $129.22\n",
      "Day 60: IBM - SELL 74 shares at $126.97\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Constants\n",
    "EPSILON = 0.1\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DECAY = 0.8\n",
    "GAMMA = 0.99\n",
    "WINDOW_SIZE = 3\n",
    "BATCH_SIZE = 32\n",
    "MEMORY_SIZE = 10000\n",
    "ACTION_SPACE = np.array([-1.0, 0.0, 1.0])  # Sell, Hold, Buy\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state):\n",
    "        self.buffer.append((state, action, reward, next_state))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class LSTMTrader(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMTrader, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, num_layers=2, dropout=0.2)\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads=4)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        attn_output, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        \n",
    "        # Get the final output\n",
    "        final_output = attn_output[:, -1, :]\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        x = self.relu(self.fc1(final_output))\n",
    "        return self.fc2(x)\n",
    "\n",
    "def add_technical_indicators(df):\n",
    "    # Calculate moving averages\n",
    "    df['SMA_5'] = df['Close'].rolling(window=5).mean()\n",
    "    df['SMA_20'] = df['Close'].rolling(window=20).mean()\n",
    "    \n",
    "    # Calculate RSI\n",
    "    delta = df['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['RSI'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Calculate MACD\n",
    "    exp1 = df['Close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['Close'].ewm(span=26, adjust=False).mean()\n",
    "    df['MACD'] = exp1 - exp2\n",
    "    df['Signal_Line'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "    \n",
    "    # Calculate Bollinger Bands\n",
    "    df['BB_middle'] = df['Close'].rolling(window=20).mean()\n",
    "    df['BB_upper'] = df['BB_middle'] + 2 * df['Close'].rolling(window=20).std()\n",
    "    df['BB_lower'] = df['BB_middle'] - 2 * df['Close'].rolling(window=20).std()\n",
    "    \n",
    "    # Add volume indicators\n",
    "    df['Volume_SMA'] = df['Volume'].rolling(window=5).mean()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_data(ticker, start_date, end_date):\n",
    "    df = yf.download(ticker, start=start_date, end=end_date)\n",
    "    df = add_technical_indicators(df)\n",
    "    df['Returns'] = df['Close'].pct_change()\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def prepare_state(df, current_idx, window_size):\n",
    "    \"\"\"Prepare the state with technical indicators\"\"\"\n",
    "    if current_idx < window_size:\n",
    "        return None\n",
    "    \n",
    "    state = []\n",
    "    for i in range(current_idx - window_size, current_idx):\n",
    "        features = [\n",
    "            df['Close'].iloc[i],\n",
    "            df['SMA_5'].iloc[i],\n",
    "            df['SMA_20'].iloc[i],\n",
    "            df['RSI'].iloc[i],\n",
    "            df['MACD'].iloc[i],\n",
    "            df['Signal_Line'].iloc[i],\n",
    "            df['BB_upper'].iloc[i],\n",
    "            df['BB_lower'].iloc[i],\n",
    "            df['Volume_SMA'].iloc[i],\n",
    "            df['Returns'].iloc[i]\n",
    "        ]\n",
    "        state.append(features)\n",
    "    return np.array(state)\n",
    "\n",
    "def train_model(model, ticker, data, replay_buffer, optimizer, criterion):\n",
    "    epsilon = EPSILON\n",
    "    \n",
    "    for episode in range(100):  # Number of episodes\n",
    "        total_reward = 0\n",
    "        state = prepare_state(data, WINDOW_SIZE, WINDOW_SIZE)\n",
    "        \n",
    "        for t in range(WINDOW_SIZE, len(data) - 1):\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            \n",
    "            # Epsilon-greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action_idx = random.randrange(len(ACTION_SPACE))\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_values = model(state_tensor)\n",
    "                    action_idx = q_values.max(1)[1].item()\n",
    "            \n",
    "            action = ACTION_SPACE[action_idx]\n",
    "            \n",
    "            # Get next state and reward\n",
    "            next_state = prepare_state(data, t + 1, WINDOW_SIZE)\n",
    "            reward = data['Returns'].iloc[t + 1] * action  # Reward based on return and action\n",
    "            \n",
    "            # Store transition in replay buffer\n",
    "            replay_buffer.push(state, action_idx, reward, next_state)\n",
    "            \n",
    "            # Train on random batch from replay buffer\n",
    "            if len(replay_buffer) > BATCH_SIZE:\n",
    "                batch = replay_buffer.sample(BATCH_SIZE)\n",
    "                state_batch = torch.FloatTensor([s[0] for s in batch])\n",
    "                action_batch = torch.LongTensor([s[1] for s in batch])\n",
    "                reward_batch = torch.FloatTensor([s[2] for s in batch])\n",
    "                next_state_batch = torch.FloatTensor([s[3] for s in batch])\n",
    "                \n",
    "                # Compute Q values\n",
    "                current_q_values = model(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "                next_q_values = model(next_state_batch).max(1)[0].detach()\n",
    "                target_q_values = reward_batch + GAMMA * next_q_values\n",
    "                \n",
    "                # Compute loss and update model\n",
    "                loss = criterion(current_q_values.squeeze(), target_q_values)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "        # Decay epsilon\n",
    "        epsilon = max(EPSILON_MIN, epsilon * EPSILON_DECAY)\n",
    "        \n",
    "        print(f\"Episode {episode + 1}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.2f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "class EnsembleTrader:\n",
    "    def __init__(self, models, data):\n",
    "        self.models = models\n",
    "        self.data = data\n",
    "        \n",
    "    def get_ensemble_action(self, state, ticker):\n",
    "        \"\"\"Get action using ensemble of models\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_values = self.models[ticker](state_tensor)\n",
    "        return ACTION_SPACE[q_values.max(1)[1].item()]\n",
    "    \n",
    "    def calculate_portfolio_value(self, cash, shares, current_prices):\n",
    "        \"\"\"Calculate total portfolio value\"\"\"\n",
    "        value = cash\n",
    "        for ticker in shares:\n",
    "            value += shares[ticker] * current_prices[ticker]\n",
    "        return value\n",
    "    \n",
    "    def simulate_trading(self, initial_cash=10000, commission=0.001):\n",
    "        cash = initial_cash\n",
    "        shares = {ticker: 0 for ticker in self.models.keys()}\n",
    "        portfolio_values = []\n",
    "        trades = []\n",
    "        \n",
    "        for t in range(WINDOW_SIZE, len(self.data)):\n",
    "            # Get the current prices for each ticker, only if there are enough rows\n",
    "            current_prices = {}\n",
    "            for ticker in self.models.keys():\n",
    "                ticker_data = self.data[self.data['Ticker'] == ticker]\n",
    "                if t < len(ticker_data):  # Ensure t is within bounds\n",
    "                    current_prices[ticker] = ticker_data['Close'].iloc[t]\n",
    "                else:\n",
    "                    current_prices[ticker] = None  # If out of bounds, set to None\n",
    "\n",
    "            # Skip if any ticker’s data is unavailable at index t\n",
    "            if any(price is None for price in current_prices.values()):\n",
    "                continue\n",
    "\n",
    "            current_prices = {ticker: self.data[self.data['Ticker'] == ticker]['Close'].iloc[t] \n",
    "                            for ticker in self.models.keys()}\n",
    "            \n",
    "            for ticker in self.models.keys():\n",
    "                ticker_data = self.data[self.data['Ticker'] == ticker]\n",
    "                state = prepare_state(ticker_data, t, WINDOW_SIZE)\n",
    "                \n",
    "                if state is not None:\n",
    "                    action = self.get_ensemble_action(state, ticker)\n",
    "                    \n",
    "                    # Calculate maximum shares that can be bought\n",
    "                    max_shares = int(cash / (current_prices[ticker] * (1 + commission)))\n",
    "                    \n",
    "                    if action > 0 and max_shares > 0:  # Buy\n",
    "                        shares_to_buy = max_shares\n",
    "                        cost = shares_to_buy * current_prices[ticker] * (1 + commission)\n",
    "                        if cost <= cash:\n",
    "                            cash -= cost\n",
    "                            shares[ticker] += shares_to_buy\n",
    "                            trades.append((t, ticker, 'BUY', shares_to_buy, current_prices[ticker]))\n",
    "                    \n",
    "                    elif action < 0 and shares[ticker] > 0:  # Sell\n",
    "                        shares_to_sell = shares[ticker]\n",
    "                        revenue = shares_to_sell * current_prices[ticker] * (1 - commission)\n",
    "                        cash += revenue\n",
    "                        shares[ticker] = 0\n",
    "                        trades.append((t, ticker, 'SELL', shares_to_sell, current_prices[ticker]))\n",
    "            \n",
    "            # Record portfolio value\n",
    "            portfolio_value = self.calculate_portfolio_value(cash, shares, current_prices)\n",
    "            portfolio_values.append((self.data.index[t], portfolio_value))\n",
    "        \n",
    "        return trades, portfolio_values, cash, shares\n",
    "\n",
    "def calculate_metrics(trades, portfolio_values, initial_cash):\n",
    "    \"\"\"Calculate trading metrics\"\"\"\n",
    "    if not portfolio_values:\n",
    "        return {}\n",
    "    \n",
    "    final_value = portfolio_values[-1][1]\n",
    "    returns = [(v2[1] - v1[1]) / v1[1] for v1, v2 in zip(portfolio_values[:-1], portfolio_values[1:])]\n",
    "    \n",
    "    metrics = {\n",
    "        'Final Balance': final_value,\n",
    "        'Total Return': ((final_value - initial_cash) / initial_cash) * 100,\n",
    "        'Win Rate': sum(1 for r in returns if r > 0) / len(returns) if returns else 0,\n",
    "        'Volatility': np.std(returns) if returns else 0,\n",
    "        'Sharpe Ratio': (np.mean(returns) / np.std(returns)) if returns else 0,\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Initialize parameters\n",
    "    start_date = '2022-01-01'\n",
    "    end_date = '2023-01-01'\n",
    "    tickers = ['AAPL', 'IBM']\n",
    "    initial_cash = 10000\n",
    "    \n",
    "    # Load and prepare training data\n",
    "    combined_data = pd.concat([load_data(ticker, start_date, end_date).assign(Ticker=ticker) \n",
    "                             for ticker in tickers])\n",
    "    \n",
    "    # Initialize models and training components\n",
    "    models = {}\n",
    "    for ticker in tickers:\n",
    "        model = LSTMTrader(input_size=10, hidden_size=64, output_size=len(ACTION_SPACE))\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.MSELoss()\n",
    "        replay_buffer = ReplayBuffer(MEMORY_SIZE)\n",
    "        \n",
    "        # Train model\n",
    "        print(f\"\\nTraining model for {ticker}...\")\n",
    "        ticker_data = combined_data[combined_data['Ticker'] == ticker].copy()\n",
    "        models[ticker] = train_model(model, ticker, ticker_data, replay_buffer, optimizer, criterion)\n",
    "\n",
    "        # Save model, optimizer, and additional parameters after training\n",
    "        save_path = f\"{ticker}_EnsembleModel.pth\"\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'criterion': criterion,\n",
    "            'replay_buffer': replay_buffer,\n",
    "            'epsilon': EPSILON\n",
    "        }, save_path)\n",
    "        print(f\"Model, optimizer, and parameters for {ticker} saved to {save_path}\")\n",
    "        \n",
    "    # Set testing period\n",
    "    test_start_date = '2023-01-01'\n",
    "    test_end_date = '2024-01-01'\n",
    "    \n",
    "    # Load and prepare testing data\n",
    "    combined_test_data = pd.concat([load_data(ticker, test_start_date, test_end_date).assign(Ticker=ticker) \n",
    "                                    for ticker in tickers])\n",
    "    \n",
    "    # Initialize ensemble trader and run testing simulation\n",
    "    print(\"\\nRunning trading simulation on test period...\")\n",
    "    ensemble_trader = EnsembleTrader(models, combined_test_data)\n",
    "    trades, portfolio_values, final_cash, final_shares = ensemble_trader.simulate_trading(initial_cash)\n",
    "    \n",
    "    # Calculate and display evaluation metrics for test period\n",
    "    metrics = calculate_metrics(trades, portfolio_values, initial_cash)\n",
    "    \n",
    "    print(\"\\nTesting Results (Evaluation Metrics):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Final Balance: ${metrics['Final Balance']:.2f}\")\n",
    "    print(f\"Total Returns: {metrics['Total Return']:.2f}%\")\n",
    "    print(f\"Win Rate: {metrics['Win Rate']:.2f}\")\n",
    "    print(f\"Volatility: {metrics['Volatility']:.4f}\")\n",
    "    print(f\"Sharpe Ratio: {metrics['Sharpe Ratio']:.4f}\")\n",
    "    \n",
    "    print(\"\\nFinal Positions:\")\n",
    "    for ticker, shares in final_shares.items():\n",
    "        print(f\"{ticker}: {shares} shares\")\n",
    "    print(f\"Cash: ${final_cash:.2f}\")\n",
    "    \n",
    "    print(\"\\nTrading History (first 10 trades in test period):\")\n",
    "    for t in trades[:10]:\n",
    "        print(f\"Day {t[0]}: {t[1]} - {t[2]} {t[3]} shares at ${t[4]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trading Results:\n",
      "==================================================\n",
      "Final Balance: $11582.65\n",
      "Total Returns: 15.83%\n",
      "Win Rate: 0.43\n",
      "Volatility: 0.0089\n",
      "Sharpe Ratio: 0.0774\n"
     ]
    }
   ],
   "source": [
    "metrics = calculate_metrics(trades, portfolio_values, initial_cash)\n",
    "\n",
    "print(\"\\nTrading Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Final Balance: ${metrics['Final Balance']:.2f}\")\n",
    "print(f\"Total Returns: {metrics['Total Return']:.2f}%\")\n",
    "print(f\"Win Rate: {metrics['Win Rate']:.2f}\")\n",
    "print(f\"Volatility: {metrics['Volatility']:.4f}\")\n",
    "print(f\"Sharpe Ratio: {metrics['Sharpe Ratio']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mnoch\\AppData\\Local\\Temp\\ipykernel_18072\\3010858531.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running trading simulation with loaded models...\n",
      "\n",
      "Trading Results:\n",
      "==================================================\n",
      "Final Balance: $7538.77\n",
      "Total Returns: -24.61%\n",
      "Win Rate: 0.48\n",
      "Volatility: 0.0225\n",
      "Sharpe Ratio: -0.6924\n",
      "\n",
      "Final Positions:\n",
      "AAPL: 58 shares\n",
      "IBM: 0 shares\n",
      "Cash: $2.83\n",
      "\n",
      "Trading History (first 10 trades):\n",
      "Day 3: AAPL - BUY 57 shares at $172.90\n",
      "Day 92: AAPL - BUY 1 shares at $131.88\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load model function\n",
    "def load_model(ticker, input_size, hidden_size, output_size, path):\n",
    "    model = LSTMTrader(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    checkpoint = torch.load(path)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    replay_buffer = checkpoint['replay_buffer']\n",
    "    epsilon = checkpoint['epsilon']\n",
    "    \n",
    "    return model, optimizer, replay_buffer, epsilon\n",
    "\n",
    "# Initialize parameters\n",
    "input_size = 10 \n",
    "hidden_size = 64\n",
    "output_size = len(ACTION_SPACE)\n",
    "tickers = ['AAPL', 'IBM']\n",
    "models = {}\n",
    "\n",
    "# Load saved models for each ticker\n",
    "for ticker in tickers:\n",
    "    path = f\"{ticker}_model.pth\"\n",
    "    model, optimizer, replay_buffer, epsilon = load_model(ticker, input_size, hidden_size, output_size, path)\n",
    "    models[ticker] = model  # Add loaded model to the models dictionary\n",
    "\n",
    "# Run the trading simulation\n",
    "# Ensure `combined_data` has been loaded and processed as before\n",
    "ensemble_trader = EnsembleTrader(models, combined_data)\n",
    "initial_cash = 10000\n",
    "\n",
    "print(\"\\nRunning trading simulation with loaded models...\")\n",
    "trades, portfolio_values, final_cash, final_shares = ensemble_trader.simulate_trading(initial_cash)\n",
    "\n",
    "# Calculate and display metrics\n",
    "metrics = calculate_metrics(trades, portfolio_values, initial_cash)\n",
    "\n",
    "print(\"\\nTrading Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Final Balance: ${metrics['Final Balance']:.2f}\")\n",
    "print(f\"Total Returns: {metrics['Total Return']:.2f}%\")\n",
    "print(f\"Win Rate: {metrics['Win Rate']:.2f}\")\n",
    "print(f\"Volatility: {metrics['Volatility']:.4f}\")\n",
    "print(f\"Sharpe Ratio: {metrics['Sharpe Ratio']:.4f}\")\n",
    "\n",
    "print(\"\\nFinal Positions:\")\n",
    "for ticker, shares in final_shares.items():\n",
    "    print(f\"{ticker}: {shares} shares\")\n",
    "print(f\"Cash: ${final_cash:.2f}\")\n",
    "\n",
    "print(\"\\nTrading History (first 10 trades):\")\n",
    "for t in trades[:]:\n",
    "    print(f\"Day {t[0]}: {t[1]} - {t[2]} {t[3]} shares at ${t[4]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running trading simulation on test period...\n",
      "\n",
      "Testing Results (Evaluation Metrics):\n",
      "==================================================\n",
      "Final Balance: $12424.03\n",
      "Total Returns: 24.24%\n",
      "Win Rate: 0.54\n",
      "Volatility: 0.0120\n",
      "Sharpe Ratio: 1.3630\n",
      "\n",
      "Final Positions:\n",
      "AAPL: 64 shares\n",
      "IBM: 0 shares\n",
      "Cash: $102.11\n",
      "\n",
      "Trading History (first 10 trades in test period):\n",
      "Day 3: AAPL - BUY 64 shares at $154.50\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Initialize parameters\n",
    "    start_date = '2022-01-01'\n",
    "    end_date = '2023-01-01'\n",
    "    tickers = ['AAPL', 'IBM']\n",
    "    initial_cash = 10000\n",
    "    \n",
    "    # Load and prepare training data\n",
    "    combined_data = pd.concat([load_data(ticker, start_date, end_date).assign(Ticker=ticker) \n",
    "                             for ticker in tickers])\n",
    "    \n",
    "    # Initialize models and training components\n",
    "    models = {}\n",
    "    for ticker in tickers:\n",
    "        model = LSTMTrader(input_size=10, hidden_size=64, output_size=len(ACTION_SPACE))\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.MSELoss()\n",
    "        replay_buffer = ReplayBuffer(MEMORY_SIZE)\n",
    "        \n",
    "        # Train model\n",
    "        print(f\"\\nTraining model for {ticker}...\")\n",
    "        ticker_data = combined_data[combined_data['Ticker'] == ticker].copy()\n",
    "        models[ticker] = train_model(model, ticker, ticker_data, replay_buffer, optimizer, criterion)\n",
    "\n",
    "        # Save model, optimizer, and additional parameters after training\n",
    "        save_path = f\"{ticker}_EnsembleModel.pth\"\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'criterion': criterion,\n",
    "            'replay_buffer': replay_buffer,\n",
    "            'epsilon': EPSILON\n",
    "        }, save_path)\n",
    "        print(f\"Model, optimizer, and parameters for {ticker} saved to {save_path}\")\n",
    "        \n",
    "    # Set testing period\n",
    "    test_start_date = '2023-01-01'\n",
    "    test_end_date = '2024-01-01'\n",
    "    \n",
    "    # Load and prepare testing data\n",
    "    combined_test_data = pd.concat([load_data(ticker, test_start_date, test_end_date).assign(Ticker=ticker) \n",
    "                                    for ticker in tickers])\n",
    "    \n",
    "    # Initialize ensemble trader and run testing simulation\n",
    "    print(\"\\nRunning trading simulation on test period...\")\n",
    "    ensemble_trader = EnsembleTrader(models, combined_test_data)\n",
    "    trades, portfolio_values, final_cash, final_shares = ensemble_trader.simulate_trading(initial_cash)\n",
    "    \n",
    "    # Calculate and display evaluation metrics for test period\n",
    "    metrics = calculate_metrics(trades, portfolio_values, initial_cash)\n",
    "    \n",
    "    print(\"\\nTesting Results (Evaluation Metrics):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Final Balance: ${metrics['Final Balance']:.2f}\")\n",
    "    print(f\"Total Returns: {metrics['Total Return']:.2f}%\")\n",
    "    print(f\"Win Rate: {metrics['Win Rate']:.2f}\")\n",
    "    print(f\"Volatility: {metrics['Volatility']:.4f}\")\n",
    "    print(f\"Sharpe Ratio: {metrics['Sharpe Ratio']:.4f}\")\n",
    "    \n",
    "    print(\"\\nFinal Positions:\")\n",
    "    for ticker, shares in final_shares.items():\n",
    "        print(f\"{ticker}: {shares} shares\")\n",
    "    print(f\"Cash: ${final_cash:.2f}\")\n",
    "    \n",
    "    print(\"\\nTrading History (first 10 trades in test period):\")\n",
    "    for t in trades[:10]:\n",
    "        print(f\"Day {t[0]}: {t[1]} - {t[2]} {t[3]} shares at ${t[4]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for AAPL...\n",
      "Episode 1, Total Reward: -0.04, Epsilon: 0.08\n",
      "Episode 2, Total Reward: 0.00, Epsilon: 0.06\n",
      "Episode 3, Total Reward: 0.29, Epsilon: 0.05\n",
      "Episode 4, Total Reward: -0.02, Epsilon: 0.04\n",
      "Episode 5, Total Reward: 0.37, Epsilon: 0.03\n",
      "Episode 6, Total Reward: -0.66, Epsilon: 0.03\n",
      "Episode 7, Total Reward: -0.67, Epsilon: 0.02\n",
      "Episode 8, Total Reward: -0.13, Epsilon: 0.02\n",
      "Episode 9, Total Reward: 0.58, Epsilon: 0.01\n",
      "Episode 10, Total Reward: 0.24, Epsilon: 0.01\n",
      "Episode 11, Total Reward: 0.06, Epsilon: 0.01\n",
      "Episode 12, Total Reward: 0.48, Epsilon: 0.01\n",
      "Episode 13, Total Reward: -0.09, Epsilon: 0.01\n",
      "Episode 14, Total Reward: -0.01, Epsilon: 0.01\n",
      "Episode 15, Total Reward: -0.60, Epsilon: 0.01\n",
      "Episode 16, Total Reward: -0.42, Epsilon: 0.01\n",
      "Episode 17, Total Reward: -0.01, Epsilon: 0.01\n",
      "Episode 18, Total Reward: 0.90, Epsilon: 0.01\n",
      "Episode 19, Total Reward: 0.19, Epsilon: 0.01\n",
      "Episode 20, Total Reward: 0.59, Epsilon: 0.01\n",
      "Episode 21, Total Reward: -0.69, Epsilon: 0.01\n",
      "Episode 22, Total Reward: -0.24, Epsilon: 0.01\n",
      "Episode 23, Total Reward: -0.23, Epsilon: 0.01\n",
      "Episode 24, Total Reward: 0.67, Epsilon: 0.01\n",
      "Episode 25, Total Reward: -0.23, Epsilon: 0.01\n",
      "Episode 26, Total Reward: -0.04, Epsilon: 0.01\n",
      "Episode 27, Total Reward: 0.39, Epsilon: 0.01\n",
      "Episode 28, Total Reward: -0.82, Epsilon: 0.01\n",
      "Episode 29, Total Reward: 0.15, Epsilon: 0.01\n",
      "Episode 30, Total Reward: -0.53, Epsilon: 0.01\n",
      "Episode 31, Total Reward: 0.12, Epsilon: 0.01\n",
      "Episode 32, Total Reward: 0.20, Epsilon: 0.01\n",
      "Episode 33, Total Reward: -0.39, Epsilon: 0.01\n",
      "Episode 34, Total Reward: -0.52, Epsilon: 0.01\n",
      "Episode 35, Total Reward: 0.25, Epsilon: 0.01\n",
      "Episode 36, Total Reward: 0.66, Epsilon: 0.01\n",
      "Episode 37, Total Reward: 0.76, Epsilon: 0.01\n",
      "Episode 38, Total Reward: -0.59, Epsilon: 0.01\n",
      "Episode 39, Total Reward: 0.12, Epsilon: 0.01\n",
      "Episode 40, Total Reward: -0.39, Epsilon: 0.01\n",
      "Episode 41, Total Reward: -0.08, Epsilon: 0.01\n",
      "Episode 42, Total Reward: 0.20, Epsilon: 0.01\n",
      "Episode 43, Total Reward: -0.15, Epsilon: 0.01\n",
      "Episode 44, Total Reward: -0.61, Epsilon: 0.01\n",
      "Episode 45, Total Reward: -0.39, Epsilon: 0.01\n",
      "Episode 46, Total Reward: 0.23, Epsilon: 0.01\n",
      "Episode 47, Total Reward: 0.07, Epsilon: 0.01\n",
      "Episode 48, Total Reward: -0.74, Epsilon: 0.01\n",
      "Episode 49, Total Reward: -0.97, Epsilon: 0.01\n",
      "Episode 50, Total Reward: -0.86, Epsilon: 0.01\n",
      "Episode 51, Total Reward: 0.35, Epsilon: 0.01\n",
      "Episode 52, Total Reward: 0.34, Epsilon: 0.01\n",
      "Episode 53, Total Reward: -0.37, Epsilon: 0.01\n",
      "Episode 54, Total Reward: -0.04, Epsilon: 0.01\n",
      "Episode 55, Total Reward: 0.99, Epsilon: 0.01\n",
      "Episode 56, Total Reward: -0.43, Epsilon: 0.01\n",
      "Episode 57, Total Reward: 0.73, Epsilon: 0.01\n",
      "Episode 58, Total Reward: 0.17, Epsilon: 0.01\n",
      "Episode 59, Total Reward: -0.74, Epsilon: 0.01\n",
      "Episode 60, Total Reward: 0.22, Epsilon: 0.01\n",
      "Episode 61, Total Reward: 0.07, Epsilon: 0.01\n",
      "Episode 62, Total Reward: -0.24, Epsilon: 0.01\n",
      "Episode 63, Total Reward: 0.08, Epsilon: 0.01\n",
      "Episode 64, Total Reward: -0.08, Epsilon: 0.01\n",
      "Episode 65, Total Reward: 0.18, Epsilon: 0.01\n",
      "Episode 66, Total Reward: 0.10, Epsilon: 0.01\n",
      "Episode 67, Total Reward: -0.36, Epsilon: 0.01\n",
      "Episode 68, Total Reward: -0.12, Epsilon: 0.01\n",
      "Episode 69, Total Reward: -0.34, Epsilon: 0.01\n",
      "Episode 70, Total Reward: -0.42, Epsilon: 0.01\n",
      "Episode 71, Total Reward: -0.08, Epsilon: 0.01\n",
      "Episode 72, Total Reward: 0.21, Epsilon: 0.01\n",
      "Episode 73, Total Reward: -0.32, Epsilon: 0.01\n",
      "Episode 74, Total Reward: -0.21, Epsilon: 0.01\n",
      "Episode 75, Total Reward: 0.05, Epsilon: 0.01\n",
      "Episode 76, Total Reward: 0.99, Epsilon: 0.01\n",
      "Episode 77, Total Reward: -0.38, Epsilon: 0.01\n",
      "Episode 78, Total Reward: 0.37, Epsilon: 0.01\n",
      "Episode 79, Total Reward: -0.01, Epsilon: 0.01\n",
      "Episode 80, Total Reward: 0.46, Epsilon: 0.01\n",
      "Episode 81, Total Reward: 0.82, Epsilon: 0.01\n",
      "Episode 82, Total Reward: 0.73, Epsilon: 0.01\n",
      "Episode 83, Total Reward: 0.10, Epsilon: 0.01\n",
      "Episode 84, Total Reward: 0.99, Epsilon: 0.01\n",
      "Episode 85, Total Reward: 0.75, Epsilon: 0.01\n",
      "Episode 86, Total Reward: 0.01, Epsilon: 0.01\n",
      "Episode 87, Total Reward: 0.41, Epsilon: 0.01\n",
      "Episode 88, Total Reward: -0.16, Epsilon: 0.01\n",
      "Episode 89, Total Reward: 1.24, Epsilon: 0.01\n",
      "Episode 90, Total Reward: -0.26, Epsilon: 0.01\n",
      "Episode 91, Total Reward: 0.28, Epsilon: 0.01\n",
      "Episode 92, Total Reward: 0.72, Epsilon: 0.01\n",
      "Episode 93, Total Reward: -0.06, Epsilon: 0.01\n",
      "Episode 94, Total Reward: 0.06, Epsilon: 0.01\n",
      "Episode 95, Total Reward: 0.62, Epsilon: 0.01\n",
      "Episode 96, Total Reward: -0.41, Epsilon: 0.01\n",
      "Episode 97, Total Reward: -0.56, Epsilon: 0.01\n",
      "Episode 98, Total Reward: 0.45, Epsilon: 0.01\n",
      "Episode 99, Total Reward: 1.39, Epsilon: 0.01\n",
      "Episode 100, Total Reward: -0.15, Epsilon: 0.01\n",
      "Model, optimizer, and parameters for AAPL saved to AAPL_EnsembleModelSoftmax.pth\n",
      "\n",
      "Training model for IBM...\n",
      "Episode 1, Total Reward: -0.20, Epsilon: 0.08\n",
      "Episode 2, Total Reward: 0.24, Epsilon: 0.06\n",
      "Episode 3, Total Reward: 0.06, Epsilon: 0.05\n",
      "Episode 4, Total Reward: 0.46, Epsilon: 0.04\n",
      "Episode 5, Total Reward: -0.67, Epsilon: 0.03\n",
      "Episode 6, Total Reward: 0.07, Epsilon: 0.03\n",
      "Episode 7, Total Reward: 0.01, Epsilon: 0.02\n",
      "Episode 8, Total Reward: 0.36, Epsilon: 0.02\n",
      "Episode 9, Total Reward: 0.23, Epsilon: 0.01\n",
      "Episode 10, Total Reward: 0.24, Epsilon: 0.01\n",
      "Episode 11, Total Reward: -0.04, Epsilon: 0.01\n",
      "Episode 12, Total Reward: 0.42, Epsilon: 0.01\n",
      "Episode 13, Total Reward: 0.61, Epsilon: 0.01\n",
      "Episode 14, Total Reward: -0.44, Epsilon: 0.01\n",
      "Episode 15, Total Reward: 0.47, Epsilon: 0.01\n",
      "Episode 16, Total Reward: -0.56, Epsilon: 0.01\n",
      "Episode 17, Total Reward: 0.24, Epsilon: 0.01\n",
      "Episode 18, Total Reward: 0.24, Epsilon: 0.01\n",
      "Episode 19, Total Reward: 0.10, Epsilon: 0.01\n",
      "Episode 20, Total Reward: -0.16, Epsilon: 0.01\n",
      "Episode 21, Total Reward: 0.36, Epsilon: 0.01\n",
      "Episode 22, Total Reward: -0.08, Epsilon: 0.01\n",
      "Episode 23, Total Reward: -0.32, Epsilon: 0.01\n",
      "Episode 24, Total Reward: -0.15, Epsilon: 0.01\n",
      "Episode 25, Total Reward: 0.33, Epsilon: 0.01\n",
      "Episode 26, Total Reward: -0.83, Epsilon: 0.01\n",
      "Episode 27, Total Reward: -0.02, Epsilon: 0.01\n",
      "Episode 28, Total Reward: -0.64, Epsilon: 0.01\n",
      "Episode 29, Total Reward: -0.16, Epsilon: 0.01\n",
      "Episode 30, Total Reward: -0.43, Epsilon: 0.01\n",
      "Episode 31, Total Reward: -0.47, Epsilon: 0.01\n",
      "Episode 32, Total Reward: 0.01, Epsilon: 0.01\n",
      "Episode 33, Total Reward: 0.53, Epsilon: 0.01\n",
      "Episode 34, Total Reward: 0.49, Epsilon: 0.01\n",
      "Episode 35, Total Reward: -0.16, Epsilon: 0.01\n",
      "Episode 36, Total Reward: -0.08, Epsilon: 0.01\n",
      "Episode 37, Total Reward: 0.67, Epsilon: 0.01\n",
      "Episode 38, Total Reward: -0.44, Epsilon: 0.01\n",
      "Episode 39, Total Reward: -0.00, Epsilon: 0.01\n",
      "Episode 40, Total Reward: -0.19, Epsilon: 0.01\n",
      "Episode 41, Total Reward: -0.02, Epsilon: 0.01\n",
      "Episode 42, Total Reward: -0.13, Epsilon: 0.01\n",
      "Episode 43, Total Reward: 0.27, Epsilon: 0.01\n",
      "Episode 44, Total Reward: 0.26, Epsilon: 0.01\n",
      "Episode 45, Total Reward: 0.15, Epsilon: 0.01\n",
      "Episode 46, Total Reward: 0.03, Epsilon: 0.01\n",
      "Episode 47, Total Reward: -0.24, Epsilon: 0.01\n",
      "Episode 48, Total Reward: -0.44, Epsilon: 0.01\n",
      "Episode 49, Total Reward: 0.51, Epsilon: 0.01\n",
      "Episode 50, Total Reward: 0.16, Epsilon: 0.01\n",
      "Episode 51, Total Reward: -0.28, Epsilon: 0.01\n",
      "Episode 52, Total Reward: 1.35, Epsilon: 0.01\n",
      "Episode 53, Total Reward: -0.05, Epsilon: 0.01\n",
      "Episode 54, Total Reward: -0.05, Epsilon: 0.01\n",
      "Episode 55, Total Reward: 0.14, Epsilon: 0.01\n",
      "Episode 56, Total Reward: 0.37, Epsilon: 0.01\n",
      "Episode 57, Total Reward: 0.17, Epsilon: 0.01\n",
      "Episode 58, Total Reward: -0.52, Epsilon: 0.01\n",
      "Episode 59, Total Reward: -0.25, Epsilon: 0.01\n",
      "Episode 60, Total Reward: 0.87, Epsilon: 0.01\n",
      "Episode 61, Total Reward: 0.48, Epsilon: 0.01\n",
      "Episode 62, Total Reward: -0.53, Epsilon: 0.01\n",
      "Episode 63, Total Reward: 0.10, Epsilon: 0.01\n",
      "Episode 64, Total Reward: 0.98, Epsilon: 0.01\n",
      "Episode 65, Total Reward: -0.17, Epsilon: 0.01\n",
      "Episode 66, Total Reward: -0.11, Epsilon: 0.01\n",
      "Episode 67, Total Reward: 0.24, Epsilon: 0.01\n",
      "Episode 68, Total Reward: 0.18, Epsilon: 0.01\n",
      "Episode 69, Total Reward: -0.05, Epsilon: 0.01\n",
      "Episode 70, Total Reward: 0.19, Epsilon: 0.01\n",
      "Episode 71, Total Reward: 0.22, Epsilon: 0.01\n",
      "Episode 72, Total Reward: -0.35, Epsilon: 0.01\n",
      "Episode 73, Total Reward: -0.31, Epsilon: 0.01\n",
      "Episode 74, Total Reward: 0.03, Epsilon: 0.01\n",
      "Episode 75, Total Reward: -0.21, Epsilon: 0.01\n",
      "Episode 76, Total Reward: -0.59, Epsilon: 0.01\n",
      "Episode 77, Total Reward: -0.36, Epsilon: 0.01\n",
      "Episode 78, Total Reward: 0.44, Epsilon: 0.01\n",
      "Episode 79, Total Reward: -0.59, Epsilon: 0.01\n",
      "Episode 80, Total Reward: 0.63, Epsilon: 0.01\n",
      "Episode 81, Total Reward: 0.11, Epsilon: 0.01\n",
      "Episode 82, Total Reward: 0.49, Epsilon: 0.01\n",
      "Episode 83, Total Reward: -0.23, Epsilon: 0.01\n",
      "Episode 84, Total Reward: 0.20, Epsilon: 0.01\n",
      "Episode 85, Total Reward: 0.61, Epsilon: 0.01\n",
      "Episode 86, Total Reward: 0.77, Epsilon: 0.01\n",
      "Episode 87, Total Reward: 0.08, Epsilon: 0.01\n",
      "Episode 88, Total Reward: -0.49, Epsilon: 0.01\n",
      "Episode 89, Total Reward: 0.83, Epsilon: 0.01\n",
      "Episode 90, Total Reward: 0.28, Epsilon: 0.01\n",
      "Episode 91, Total Reward: -0.28, Epsilon: 0.01\n",
      "Episode 92, Total Reward: 0.68, Epsilon: 0.01\n",
      "Episode 93, Total Reward: -0.30, Epsilon: 0.01\n",
      "Episode 94, Total Reward: 0.35, Epsilon: 0.01\n",
      "Episode 95, Total Reward: 0.67, Epsilon: 0.01\n",
      "Episode 96, Total Reward: -0.41, Epsilon: 0.01\n",
      "Episode 97, Total Reward: 0.35, Epsilon: 0.01\n",
      "Episode 98, Total Reward: -0.11, Epsilon: 0.01\n",
      "Episode 99, Total Reward: 0.00, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Total Reward: -0.15, Epsilon: 0.01\n",
      "Model, optimizer, and parameters for IBM saved to IBM_EnsembleModelSoftmax.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running trading simulation on test period...\n",
      "\n",
      "Testing Results (Evaluation Metrics):\n",
      "==================================================\n",
      "Final Balance: $12802.79\n",
      "Total Returns: 28.03%\n",
      "Win Rate: 0.38\n",
      "Volatility: 0.0087\n",
      "Sharpe Ratio: 0.1302\n",
      "\n",
      "Final Positions:\n",
      "AAPL: 0 shares\n",
      "IBM: 0 shares\n",
      "Cash: $12802.79\n",
      "\n",
      "Trading History (first 10 trades in test period):\n",
      "Day 3: IBM - BUY 72 shares at $136.94\n",
      "Day 5: IBM - SELL 72 shares at $135.84\n",
      "Day 6: IBM - BUY 72 shares at $135.98\n",
      "Day 9: IBM - SELL 72 shares at $137.35\n",
      "Day 15: IBM - BUY 76 shares at $130.97\n",
      "Day 17: IBM - SELL 76 shares at $130.57\n",
      "Day 19: IBM - BUY 76 shares at $129.30\n",
      "Day 21: IBM - SELL 76 shares at $128.93\n",
      "Day 22: AAPL - BUY 65 shares at $151.03\n",
      "Day 23: AAPL - SELL 65 shares at $153.83\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "# Constants\n",
    "EPSILON = 0.1\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DECAY = 0.8\n",
    "GAMMA = 0.99\n",
    "WINDOW_SIZE = 3\n",
    "BATCH_SIZE = 32\n",
    "MEMORY_SIZE = 10000\n",
    "ACTION_SPACE = np.array([-1.0, 0.0, 1.0])  # Sell, Hold, Buy\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state):\n",
    "        self.buffer.append((state, action, reward, next_state))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class LSTMTrader(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMTrader, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, num_layers=2, dropout=0.2)\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads=4)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        attn_output, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        \n",
    "        # Get the final output\n",
    "        final_output = attn_output[:, -1, :]\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        x = self.relu(self.fc1(final_output))\n",
    "        return self.fc2(x)\n",
    "\n",
    "def add_technical_indicators(df):\n",
    "    # Calculate moving averages\n",
    "    df['SMA_5'] = df['Close'].rolling(window=5).mean()\n",
    "    df['SMA_20'] = df['Close'].rolling(window=20).mean()\n",
    "    \n",
    "    # Calculate RSI\n",
    "    delta = df['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['RSI'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Calculate MACD\n",
    "    exp1 = df['Close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['Close'].ewm(span=26, adjust=False).mean()\n",
    "    df['MACD'] = exp1 - exp2\n",
    "    df['Signal_Line'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "    \n",
    "    # Calculate Bollinger Bands\n",
    "    df['BB_middle'] = df['Close'].rolling(window=20).mean()\n",
    "    df['BB_upper'] = df['BB_middle'] + 2 * df['Close'].rolling(window=20).std()\n",
    "    df['BB_lower'] = df['BB_middle'] - 2 * df['Close'].rolling(window=20).std()\n",
    "    \n",
    "    # Add volume indicators\n",
    "    df['Volume_SMA'] = df['Volume'].rolling(window=5).mean()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_data(ticker, start_date, end_date):\n",
    "    df = yf.download(ticker, start=start_date, end=end_date)\n",
    "    df = add_technical_indicators(df)\n",
    "    df['Returns'] = df['Close'].pct_change()\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def prepare_state(df, current_idx, window_size):\n",
    "    \"\"\"Prepare the state with technical indicators\"\"\"\n",
    "    if current_idx < window_size:\n",
    "        return None\n",
    "    \n",
    "    state = []\n",
    "    for i in range(current_idx - window_size, current_idx):\n",
    "        features = [\n",
    "            df['Close'].iloc[i],\n",
    "            df['SMA_5'].iloc[i],\n",
    "            df['SMA_20'].iloc[i],\n",
    "            df['RSI'].iloc[i],\n",
    "            df['MACD'].iloc[i],\n",
    "            df['Signal_Line'].iloc[i],\n",
    "            df['BB_upper'].iloc[i],\n",
    "            df['BB_lower'].iloc[i],\n",
    "            df['Volume_SMA'].iloc[i],\n",
    "            df['Returns'].iloc[i]\n",
    "        ]\n",
    "        state.append(features)\n",
    "    return np.array(state)\n",
    "\n",
    "def train_model(model, ticker, data, replay_buffer, optimizer, criterion):\n",
    "    epsilon = EPSILON\n",
    "    \n",
    "    for episode in range(100):  # Number of episodes\n",
    "        total_reward = 0\n",
    "        state = prepare_state(data, WINDOW_SIZE, WINDOW_SIZE)\n",
    "        \n",
    "        for t in range(WINDOW_SIZE, len(data) - 1):\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            \n",
    "            # Epsilon-greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action_idx = random.randrange(len(ACTION_SPACE))\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_values = model(state_tensor)\n",
    "                    action_idx = q_values.max(1)[1].item()\n",
    "            \n",
    "            action = ACTION_SPACE[action_idx]\n",
    "            \n",
    "            # Get next state and reward\n",
    "            next_state = prepare_state(data, t + 1, WINDOW_SIZE)\n",
    "            reward = data['Returns'].iloc[t + 1] * action  # Reward based on return and action\n",
    "            \n",
    "            # Store transition in replay buffer\n",
    "            replay_buffer.push(state, action_idx, reward, next_state)\n",
    "            \n",
    "            # Train on random batch from replay buffer\n",
    "            if len(replay_buffer) > BATCH_SIZE:\n",
    "                batch = replay_buffer.sample(BATCH_SIZE)\n",
    "                state_batch = torch.FloatTensor([s[0] for s in batch])\n",
    "                action_batch = torch.LongTensor([s[1] for s in batch])\n",
    "                reward_batch = torch.FloatTensor([s[2] for s in batch])\n",
    "                next_state_batch = torch.FloatTensor([s[3] for s in batch])\n",
    "                \n",
    "                # Compute Q values\n",
    "                current_q_values = model(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "                next_q_values = model(next_state_batch).max(1)[0].detach()\n",
    "                target_q_values = reward_batch + GAMMA * next_q_values\n",
    "                \n",
    "                # Compute loss and update model\n",
    "                loss = criterion(current_q_values.squeeze(), target_q_values)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "        # Decay epsilon\n",
    "        epsilon = max(EPSILON_MIN, epsilon * EPSILON_DECAY)\n",
    "        \n",
    "        print(f\"Episode {episode + 1}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.2f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "class EnsembleTrader:\n",
    "    def __init__(self, models, data):\n",
    "        self.models = models\n",
    "        self.data = data\n",
    "\n",
    "    def get_ensemble_action(self, state, ticker):\n",
    "        \"\"\"Get action using ensemble of models with softmax.\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_values = self.models[ticker](state_tensor)\n",
    "        \n",
    "        # Apply softmax to Q-values to get probabilities\n",
    "        probabilities = F.softmax(q_values, dim=1).detach().numpy()\n",
    "        \n",
    "        # Sample an action based on the probabilities\n",
    "        action_index = np.random.choice(len(probabilities[0]), p=probabilities[0])\n",
    "        return ACTION_SPACE[action_index]\n",
    "\n",
    "    def calculate_portfolio_value(self, cash, shares, current_prices):\n",
    "        \"\"\"Calculate total portfolio value.\"\"\"\n",
    "        value = cash\n",
    "        for ticker in shares:\n",
    "            value += shares[ticker] * current_prices[ticker]\n",
    "        return value\n",
    "\n",
    "    def simulate_trading(self, initial_cash=10000, commission=0.001):\n",
    "        cash = initial_cash\n",
    "        shares = {ticker: 0 for ticker in self.models.keys()}\n",
    "        portfolio_values = []\n",
    "        trades = []\n",
    "        \n",
    "        for t in range(WINDOW_SIZE, len(self.data)):\n",
    "            # Get the current prices for each ticker\n",
    "            current_prices = {}\n",
    "            for ticker in self.models.keys():\n",
    "                ticker_data = self.data[self.data['Ticker'] == ticker]\n",
    "                if t < len(ticker_data):  # Ensure t is within bounds\n",
    "                    current_prices[ticker] = ticker_data['Close'].iloc[t]\n",
    "                else:\n",
    "                    current_prices[ticker] = None  # If out of bounds, set to None\n",
    "\n",
    "            # Skip if any ticker’s data is unavailable at index t\n",
    "            if any(price is None for price in current_prices.values()):\n",
    "                continue\n",
    "\n",
    "            for ticker in self.models.keys():\n",
    "                ticker_data = self.data[self.data['Ticker'] == ticker]\n",
    "                state = prepare_state(ticker_data, t, WINDOW_SIZE)\n",
    "                \n",
    "                if state is not None:\n",
    "                    action = self.get_ensemble_action(state, ticker)\n",
    "                    \n",
    "                    # Calculate maximum shares that can be bought\n",
    "                    max_shares = int(cash / (current_prices[ticker] * (1 + commission)))\n",
    "                    \n",
    "                    if action > 0 and max_shares > 0:  # Buy\n",
    "                        shares_to_buy = max_shares\n",
    "                        cost = shares_to_buy * current_prices[ticker] * (1 + commission)\n",
    "                        if cost <= cash:\n",
    "                            cash -= cost\n",
    "                            shares[ticker] += shares_to_buy\n",
    "                            trades.append((t, ticker, 'BUY', shares_to_buy, current_prices[ticker]))\n",
    "                    \n",
    "                    elif action < 0 and shares[ticker] > 0:  # Sell\n",
    "                        shares_to_sell = shares[ticker]\n",
    "                        revenue = shares_to_sell * current_prices[ticker] * (1 - commission)\n",
    "                        cash += revenue\n",
    "                        shares[ticker] = 0\n",
    "                        trades.append((t, ticker, 'SELL', shares_to_sell, current_prices[ticker]))\n",
    "\n",
    "            # Record portfolio value\n",
    "            portfolio_value = self.calculate_portfolio_value(cash, shares, current_prices)\n",
    "            portfolio_values.append((self.data.index[t], portfolio_value))\n",
    "        \n",
    "        return trades, portfolio_values, cash, shares\n",
    "\n",
    "def calculate_metrics(trades, portfolio_values, initial_cash):\n",
    "    \"\"\"Calculate trading metrics\"\"\"\n",
    "    if not portfolio_values:\n",
    "        return {}\n",
    "    \n",
    "    final_value = portfolio_values[-1][1]\n",
    "    returns = [(v2[1] - v1[1]) / v1[1] for v1, v2 in zip(portfolio_values[:-1], portfolio_values[1:])]\n",
    "    \n",
    "    metrics = {\n",
    "        'Final Balance': final_value,\n",
    "        'Total Return': ((final_value - initial_cash) / initial_cash) * 100,\n",
    "        'Win Rate': sum(1 for r in returns if r > 0) / len(returns) if returns else 0,\n",
    "        'Volatility': np.std(returns) if returns else 0,\n",
    "        'Sharpe Ratio': (np.mean(returns) / np.std(returns)) if returns else 0,\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Initialize parameters\n",
    "    start_date = '2020-01-01'\n",
    "    end_date = '2023-01-01'\n",
    "    tickers = ['AAPL', 'IBM']\n",
    "    initial_cash = 10000\n",
    "    \n",
    "    # Load and prepare training data\n",
    "    combined_data = pd.concat([load_data(ticker, start_date, end_date).assign(Ticker=ticker) \n",
    "                             for ticker in tickers])\n",
    "    \n",
    "    # Initialize models and training components\n",
    "    models = {}\n",
    "    for ticker in tickers:\n",
    "        model = LSTMTrader(input_size=10, hidden_size=64, output_size=len(ACTION_SPACE))\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.MSELoss()\n",
    "        replay_buffer = ReplayBuffer(MEMORY_SIZE)\n",
    "        \n",
    "        # Train model\n",
    "        print(f\"\\nTraining model for {ticker}...\")\n",
    "        ticker_data = combined_data[combined_data['Ticker'] == ticker].copy()\n",
    "        models[ticker] = train_model(model, ticker, ticker_data, replay_buffer, optimizer, criterion)\n",
    "\n",
    "        # Save model, optimizer, and additional parameters after training\n",
    "        save_path = f\"{ticker}_EnsembleModelSoftmax.pth\"\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'criterion': criterion,\n",
    "            'replay_buffer': replay_buffer,\n",
    "            'epsilon': EPSILON\n",
    "        }, save_path)\n",
    "        print(f\"Model, optimizer, and parameters for {ticker} saved to {save_path}\")\n",
    "        \n",
    "    # Set testing period\n",
    "    test_start_date = '2023-01-01'\n",
    "    test_end_date = '2024-01-01'\n",
    "    \n",
    "    # Load and prepare testing data\n",
    "    combined_test_data = pd.concat([load_data(ticker, test_start_date, test_end_date).assign(Ticker=ticker) \n",
    "                                    for ticker in tickers])\n",
    "    \n",
    "    # Initialize ensemble trader and run testing simulation\n",
    "    print(\"\\nRunning trading simulation on test period...\")\n",
    "    ensemble_trader = EnsembleTrader(models, combined_test_data)\n",
    "    trades, portfolio_values, final_cash, final_shares = ensemble_trader.simulate_trading(initial_cash)\n",
    "    \n",
    "    # Calculate and display evaluation metrics for test period\n",
    "    metrics = calculate_metrics(trades, portfolio_values, initial_cash)\n",
    "    \n",
    "    print(\"\\nTesting Results (Evaluation Metrics):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Final Balance: ${metrics['Final Balance']:.2f}\")\n",
    "    print(f\"Total Returns: {metrics['Total Return']:.2f}%\")\n",
    "    print(f\"Win Rate: {metrics['Win Rate']:.2f}\")\n",
    "    print(f\"Volatility: {metrics['Volatility']:.4f}\")\n",
    "    print(f\"Sharpe Ratio: {metrics['Sharpe Ratio']:.4f}\")\n",
    "    \n",
    "    print(\"\\nFinal Positions:\")\n",
    "    for ticker, shares in final_shares.items():\n",
    "        print(f\"{ticker}: {shares} shares\")\n",
    "    print(f\"Cash: ${final_cash:.2f}\")\n",
    "    \n",
    "    print(\"\\nTrading History (first 10 trades in test period):\")\n",
    "    for t in trades[:10]:\n",
    "        print(f\"Day {t[0]}: {t[1]} - {t[2]} {t[3]} shares at ${t[4]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the model #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 143\u001b[0m\n\u001b[0;32m    141\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m    142\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m--> 143\u001b[0m \u001b[43mrandom\u001b[49m\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# Initialize parameters\u001b[39;00m\n\u001b[0;32m    146\u001b[0m start_date \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random  # For generating random actions\n",
    "import numpy as np  # For handling arrays and numerical operations\n",
    "import torch  # For working with PyTorch models\n",
    "import torch.nn as nn  # For defining neural network layers\n",
    "import torch.optim as optim  # For optimization\n",
    "import matplotlib.pyplot as plt  # For plotting the loss\n",
    "import pandas as pd  # For handling CSV file operations\n",
    "import gym  # For the Gym environment\n",
    "\n",
    "WINDOW_SIZE = 3\n",
    "ACTION_SPACE = np.array([-1.0, 0.0, 1.0])  # Sell, Hold, Buy\n",
    "\n",
    "def load_model(model_path):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model = LSTMTrader(input_size=10, hidden_size=64, output_size=len(ACTION_SPACE))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epsilon = checkpoint['epsilon']\n",
    "    replay_buffer = checkpoint['replay_buffer']\n",
    "    \n",
    "    return model, optimizer, epsilon, replay_buffer\n",
    "\n",
    "class EnsembleTrader:\n",
    "    def __init__(self, models, data):\n",
    "        self.models = models\n",
    "        self.data = data\n",
    "        \n",
    "    def get_ensemble_action(self, state, ticker):\n",
    "        #Get action using ensemble of models\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_values = self.models[ticker](state_tensor)\n",
    "        return ACTION_SPACE[q_values.max(1)[1].item()]\n",
    "    \n",
    "    def calculate_portfolio_value(self, cash, shares, current_prices):\n",
    "        #Calculate total portfolio value\n",
    "        value = cash\n",
    "        for ticker in shares:\n",
    "            value += shares[ticker] * current_prices[ticker]\n",
    "        return value\n",
    "    \n",
    "    def simulate_trading(self, initial_cash=10000, commission=0.001):\n",
    "        cash = initial_cash\n",
    "        shares = {ticker: 0 for ticker in self.models.keys()}\n",
    "        portfolio_values = []\n",
    "        trades = []\n",
    "        \n",
    "        for t in range(WINDOW_SIZE, len(self.data)):\n",
    "            # Get the current prices for each ticker, only if there are enough rows\n",
    "            current_prices = {}\n",
    "            for ticker in self.models.keys():\n",
    "                ticker_data = self.data[self.data['Ticker'] == ticker]\n",
    "                if t < len(ticker_data):  # Ensure t is within bounds\n",
    "                    current_prices[ticker] = ticker_data['Close'].iloc[t]\n",
    "                else:\n",
    "                    current_prices[ticker] = None  # If out of bounds, set to None\n",
    "\n",
    "            # Skip if any ticker’s data is unavailable at index t\n",
    "            if any(price is None for price in current_prices.values()):\n",
    "                continue\n",
    "\n",
    "            current_prices = {ticker: self.data[self.data['Ticker'] == ticker]['Close'].iloc[t] \n",
    "                            for ticker in self.models.keys()}\n",
    "            \n",
    "            for ticker in self.models.keys():\n",
    "                ticker_data = self.data[self.data['Ticker'] == ticker]\n",
    "                state = prepare_state(ticker_data, t, WINDOW_SIZE)\n",
    "                \n",
    "                if state is not None:\n",
    "                    action = self.get_ensemble_action(state, ticker)\n",
    "                    \n",
    "                    # Calculate maximum shares that can be bought\n",
    "                    max_shares = int(cash / (current_prices[ticker] * (1 + commission)))\n",
    "                    \n",
    "                    if action > 0 and max_shares > 0:  # Buy\n",
    "                        shares_to_buy = max_shares\n",
    "                        cost = shares_to_buy * current_prices[ticker] * (1 + commission)\n",
    "                        if cost <= cash:\n",
    "                            cash -= cost\n",
    "                            shares[ticker] += shares_to_buy\n",
    "                            trades.append((t, ticker, 'BUY', shares_to_buy, current_prices[ticker]))\n",
    "                    \n",
    "                    elif action < 0 and shares[ticker] > 0:  # Sell\n",
    "                        shares_to_sell = shares[ticker]\n",
    "                        revenue = shares_to_sell * current_prices[ticker] * (1 - commission)\n",
    "                        cash += revenue\n",
    "                        shares[ticker] = 0\n",
    "                        trades.append((t, ticker, 'SELL', shares_to_sell, current_prices[ticker]))\n",
    "            \n",
    "            # Record portfolio value\n",
    "            portfolio_value = self.calculate_portfolio_value(cash, shares, current_prices)\n",
    "            portfolio_values.append((self.data.index[t], portfolio_value))\n",
    "        \n",
    "        return trades, portfolio_values, cash, shares\n",
    "    \n",
    "def calculate_td_loss(model, state, next_state, reward, done):\n",
    "    \"\"\"\n",
    "    Calculate the Temporal Difference (TD) loss for testing.\n",
    "    \"\"\"\n",
    "    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "    next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "    \n",
    "    # Get Q-values for current state\n",
    "    q_values = model(state_tensor)\n",
    "    # Get Q-values for next state\n",
    "    next_q_values = model(next_state_tensor)\n",
    "    \n",
    "    # Take the max Q-value for the next state (the action value we're predicting for next state)\n",
    "    max_next_q_value = next_q_values.max(1)[0].detach()\n",
    "    \n",
    "    # Compute the target Q-value for this state-action pair\n",
    "    target_q_value = reward + GAMMA * max_next_q_value * (1 - done)\n",
    "    \n",
    "    # Calculate the predicted Q-value for the current state-action pair\n",
    "    current_q_value = q_values.max(1)[0]  # Best action for the current state\n",
    "    \n",
    "    # Calculate Temporal Difference (TD) loss\n",
    "    td_loss = (current_q_value - target_q_value).pow(2).mean()  # MSE loss\n",
    "    \n",
    "    return td_loss.item()\n",
    "\n",
    "def test_model_and_save_loss(models, data, file_name=\"td_loss.txt\"):\n",
    "    td_losses = []  # List to store TD loss values\n",
    "    \n",
    "    for t in range(WINDOW_SIZE, len(data) - 1):\n",
    "        state = prepare_state(data, t, WINDOW_SIZE)\n",
    "        next_state = prepare_state(data, t + 1, WINDOW_SIZE)\n",
    "        \n",
    "        if state is None or next_state is None:\n",
    "            continue\n",
    "        \n",
    "        reward = data['Returns'].iloc[t + 1]  # Reward based on return\n",
    "        done = 0  # Usually done = 0 during testing unless you have an episode end condition\n",
    "        \n",
    "        # For simplicity, we'll calculate TD loss for the first model (for now)\n",
    "        td_loss = calculate_td_loss(models['AAPL'], state, next_state, reward, done)\n",
    "        td_losses.append(td_loss)  # Append the loss to the list\n",
    "    \n",
    "    # Save the TD losses to a file (e.g., txt or csv)\n",
    "    np.savetxt(file_name, td_losses, delimiter=\",\")\n",
    "    print(f\"TD Loss saved to {file_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Initialize parameters\n",
    "    start_date = '2023-01-01'\n",
    "    end_date = '2024-01-01'\n",
    "    tickers = ['AAPL', 'IBM']\n",
    "    initial_cash = 10000\n",
    "    \n",
    "    # Load and prepare data\n",
    "    combined_data = pd.concat([load_data(ticker, start_date, end_date).assign(Ticker=ticker) \n",
    "                             for ticker in tickers])\n",
    "\n",
    "    # Load trained models for each ticker\n",
    "    models = {}\n",
    "    for ticker in tickers:\n",
    "        model_path = f\"{ticker}_model.pth\"\n",
    "        model, optimizer, epsilon, replay_buffer = load_model(model_path)\n",
    "        models[ticker] = model\n",
    "    \n",
    "    test_model_and_save_loss(models, combined_data)\n",
    "\n",
    "    # # Initialize ensemble trader and run simulation\n",
    "    # print(\"\\nRunning trading simulation...\")\n",
    "    # ensemble_trader = EnsembleTrader(models, combined_data)\n",
    "    # trades, portfolio_values, final_cash, final_shares = ensemble_trader.simulate_trading(initial_cash)\n",
    "    \n",
    "    # # Calculate and display metrics\n",
    "    # metrics = calculate_metrics(trades, portfolio_values, initial_cash)\n",
    "    \n",
    "    # print(\"\\nTrading Results:\")\n",
    "    # print(\"=\" * 50)\n",
    "    # print(f\"Final Balance: ${metrics['Final Balance']:.2f}\")\n",
    "    # print(f\"Total Returns: {metrics['Total Return']:.2f}%\")\n",
    "    # print(f\"Win Rate: {metrics['Win Rate']:.2f}\")\n",
    "    # print(f\"Volatility: {metrics['Volatility']:.4f}\")\n",
    "    # print(f\"Sharpe Ratio: {metrics['Sharpe Ratio']:.4f}\")\n",
    "    \n",
    "    # print(\"\\nFinal Positions:\")\n",
    "    # for ticker, shares in final_shares.items():\n",
    "    #     print(f\"{ticker}: {shares} shares\")\n",
    "    # print(f\"Cash: ${final_cash:.2f}\")\n",
    "    \n",
    "    # print(\"\\nTrading History (first 10 trades):\")\n",
    "    # for t in trades[:10]:\n",
    "    #     print(f\"Day {t[0]}: {t[1]} - {t[2]} {t[3]} shares at ${t[4]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting temporal losses ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the TD loss values from the CSV file\n",
    "td_losses = np.loadtxt('td_loss.txt', delimiter=\",\")\n",
    "\n",
    "# Plot the TD losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(td_losses, label=\"Temporal Difference Loss\")\n",
    "plt.xlabel('Test Steps')\n",
    "plt.ylabel('TD Loss')\n",
    "plt.title('Temporal Difference Loss over Test Steps')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Constants\n",
    "EPSILON = 0.1\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DECAY = 0.8\n",
    "GAMMA = 0.99\n",
    "WINDOW_SIZE = 3\n",
    "BATCH_SIZE = 32\n",
    "MEMORY_SIZE = 10000\n",
    "ACTION_SPACE = np.array([-1.0, 0.0, 1.0])  # Sell, Hold, Buy\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state):\n",
    "        self.buffer.append((state, action, reward, next_state))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class LSTMTrader(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMTrader, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, num_layers=2, dropout=0.2)\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads=4)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        attn_output, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        \n",
    "        # Get the final output\n",
    "        final_output = attn_output[:, -1, :]\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        x = self.relu(self.fc1(final_output))\n",
    "        return self.fc2(x)\n",
    "\n",
    "def add_technical_indicators(df):\n",
    "    # Calculate moving averages\n",
    "    df['SMA_5'] = df['Close'].rolling(window=5).mean()\n",
    "    df['SMA_20'] = df['Close'].rolling(window=20).mean()\n",
    "    \n",
    "    # Calculate RSI\n",
    "    delta = df['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['RSI'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Calculate MACD\n",
    "    exp1 = df['Close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['Close'].ewm(span=26, adjust=False).mean()\n",
    "    df['MACD'] = exp1 - exp2\n",
    "    df['Signal_Line'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "    \n",
    "    # Calculate Bollinger Bands\n",
    "    df['BB_middle'] = df['Close'].rolling(window=20).mean()\n",
    "    df['BB_upper'] = df['BB_middle'] + 2 * df['Close'].rolling(window=20).std()\n",
    "    df['BB_lower'] = df['BB_middle'] - 2 * df['Close'].rolling(window=20).std()\n",
    "    \n",
    "    # Add volume indicators\n",
    "    df['Volume_SMA'] = df['Volume'].rolling(window=5).mean()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_data(ticker, start_date, end_date):\n",
    "    df = yf.download(ticker, start=start_date, end=end_date)\n",
    "    df = add_technical_indicators(df)\n",
    "    df['Returns'] = df['Close'].pct_change()\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def prepare_state(df, current_idx, window_size):\n",
    "    \"\"\"Prepare the state with technical indicators\"\"\"\n",
    "    if current_idx < window_size:\n",
    "        return None\n",
    "    \n",
    "    state = []\n",
    "    for i in range(current_idx - window_size, current_idx):\n",
    "        features = [\n",
    "            df['Close'].iloc[i],\n",
    "            df['SMA_5'].iloc[i],\n",
    "            df['SMA_20'].iloc[i],\n",
    "            df['RSI'].iloc[i],\n",
    "            df['MACD'].iloc[i],\n",
    "            df['Signal_Line'].iloc[i],\n",
    "            df['BB_upper'].iloc[i],\n",
    "            df['BB_lower'].iloc[i],\n",
    "            df['Volume_SMA'].iloc[i],\n",
    "            df['Returns'].iloc[i]\n",
    "        ]\n",
    "        state.append(features)\n",
    "    return np.array(state)\n",
    "\n",
    "# def train_model(model, ticker, data, replay_buffer, optimizer, criterion):\n",
    "#     epsilon = EPSILON\n",
    "    \n",
    "#     for episode in range(100):  # Number of episodes\n",
    "#         total_reward = 0\n",
    "#         state = prepare_state(data, WINDOW_SIZE, WINDOW_SIZE)\n",
    "        \n",
    "#         for t in range(WINDOW_SIZE, len(data) - 1):\n",
    "#             state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            \n",
    "#             # Epsilon-greedy action selection\n",
    "#             if random.random() < epsilon:\n",
    "#                 action_idx = random.randrange(len(ACTION_SPACE))\n",
    "#             else:\n",
    "#                 with torch.no_grad():\n",
    "#                     q_values = model(state_tensor)\n",
    "#                     action_idx = q_values.max(1)[1].item()\n",
    "            \n",
    "#             action = ACTION_SPACE[action_idx]\n",
    "            \n",
    "#             # Get next state and reward\n",
    "#             next_state = prepare_state(data, t + 1, WINDOW_SIZE)\n",
    "#             reward = data['Returns'].iloc[t + 1] * action  # Reward based on return and action\n",
    "            \n",
    "#             # Store transition in replay buffer\n",
    "#             replay_buffer.push(state, action_idx, reward, next_state)\n",
    "            \n",
    "#             # Train on random batch from replay buffer\n",
    "#             if len(replay_buffer) > BATCH_SIZE:\n",
    "#                 batch = replay_buffer.sample(BATCH_SIZE)\n",
    "#                 state_batch = torch.FloatTensor([s[0] for s in batch])\n",
    "#                 action_batch = torch.LongTensor([s[1] for s in batch])\n",
    "#                 reward_batch = torch.FloatTensor([s[2] for s in batch])\n",
    "#                 next_state_batch = torch.FloatTensor([s[3] for s in batch])\n",
    "                \n",
    "#                 # Compute Q values\n",
    "#                 current_q_values = model(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "#                 next_q_values = model(next_state_batch).max(1)[0].detach()\n",
    "#                 target_q_values = reward_batch + GAMMA * next_q_values\n",
    "                \n",
    "#                 # Compute loss and update model\n",
    "#                 loss = criterion(current_q_values.squeeze(), target_q_values)\n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "            \n",
    "#             state = next_state\n",
    "#             total_reward += reward\n",
    "            \n",
    "#         # Decay epsilon\n",
    "#         epsilon = max(EPSILON_MIN, epsilon * EPSILON_DECAY)\n",
    "        \n",
    "#         print(f\"Episode {episode + 1}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.2f}\")\n",
    "    \n",
    "#     return model\n",
    "\n",
    "class EnsembleTrader:\n",
    "    def __init__(self, models, data):\n",
    "        self.models = models\n",
    "        self.data = data\n",
    "        \n",
    "    def get_ensemble_action(self, state, ticker):\n",
    "        \"\"\"Get action using ensemble of models\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_values = self.models[ticker](state_tensor)\n",
    "        return ACTION_SPACE[q_values.max(1)[1].item()]\n",
    "    \n",
    "    def calculate_portfolio_value(self, cash, shares, current_prices):\n",
    "        \"\"\"Calculate total portfolio value\"\"\"\n",
    "        value = cash\n",
    "        for ticker in shares:\n",
    "            value += shares[ticker] * current_prices[ticker]\n",
    "        return value\n",
    "    \n",
    "    def simulate_trading(self, initial_cash=10000, commission=0.001):\n",
    "        cash = initial_cash\n",
    "        shares = {ticker: 0 for ticker in self.models.keys()}\n",
    "        portfolio_values = []\n",
    "        trades = []\n",
    "        \n",
    "        for t in range(WINDOW_SIZE, len(self.data)):\n",
    "            # Get the current prices for each ticker, only if there are enough rows\n",
    "            current_prices = {}\n",
    "            for ticker in self.models.keys():\n",
    "                ticker_data = self.data[self.data['Ticker'] == ticker]\n",
    "                if t < len(ticker_data):  # Ensure t is within bounds\n",
    "                    current_prices[ticker] = ticker_data['Close'].iloc[t]\n",
    "                else:\n",
    "                    current_prices[ticker] = None  # If out of bounds, set to None\n",
    "\n",
    "            # Skip if any ticker’s data is unavailable at index t\n",
    "            if any(price is None for price in current_prices.values()):\n",
    "                continue\n",
    "\n",
    "            current_prices = {ticker: self.data[self.data['Ticker'] == ticker]['Close'].iloc[t] \n",
    "                            for ticker in self.models.keys()}\n",
    "            \n",
    "            for ticker in self.models.keys():\n",
    "                ticker_data = self.data[self.data['Ticker'] == ticker]\n",
    "                state = prepare_state(ticker_data, t, WINDOW_SIZE)\n",
    "                \n",
    "                if state is not None:\n",
    "                    action = self.get_ensemble_action(state, ticker)\n",
    "                    \n",
    "                    # Calculate maximum shares that can be bought\n",
    "                    max_shares = int(cash / (current_prices[ticker] * (1 + commission)))\n",
    "                    \n",
    "                    if action > 0 and max_shares > 0:  # Buy\n",
    "                        shares_to_buy = max_shares\n",
    "                        cost = shares_to_buy * current_prices[ticker] * (1 + commission)\n",
    "                        if cost <= cash:\n",
    "                            cash -= cost\n",
    "                            shares[ticker] += shares_to_buy\n",
    "                            trades.append((t, ticker, 'BUY', shares_to_buy, current_prices[ticker]))\n",
    "                    \n",
    "                    elif action < 0 and shares[ticker] > 0:  # Sell\n",
    "                        shares_to_sell = shares[ticker]\n",
    "                        revenue = shares_to_sell * current_prices[ticker] * (1 - commission)\n",
    "                        cash += revenue\n",
    "                        shares[ticker] = 0\n",
    "                        trades.append((t, ticker, 'SELL', shares_to_sell, current_prices[ticker]))\n",
    "            \n",
    "            # Record portfolio value\n",
    "            portfolio_value = self.calculate_portfolio_value(cash, shares, current_prices)\n",
    "            portfolio_values.append((self.data.index[t], portfolio_value))\n",
    "        \n",
    "        return trades, portfolio_values, cash, shares\n",
    "\n",
    "def calculate_metrics(trades, portfolio_values, initial_cash):\n",
    "    \"\"\"Calculate trading metrics\"\"\"\n",
    "    if not portfolio_values:\n",
    "        return {}\n",
    "    \n",
    "    final_value = portfolio_values[-1][1]\n",
    "    returns = [(v2[1] - v1[1]) / v1[1] for v1, v2 in zip(portfolio_values[:-1], portfolio_values[1:])]\n",
    "    \n",
    "    metrics = {\n",
    "        'Final Balance': final_value,\n",
    "        'Total Return': ((final_value - initial_cash) / initial_cash) * 100,\n",
    "        'Win Rate': sum(1 for r in returns if r > 0) / len(returns) if returns else 0,\n",
    "        'Volatility': np.std(returns) if returns else 0,\n",
    "        'Sharpe Ratio': (np.mean(returns) / np.std(returns)) * np.sqrt(252) if returns else 0,\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Initialize parameters\n",
    "    start_date = '2022-01-01'\n",
    "    end_date = '2023-01-01'\n",
    "    tickers = ['AAPL', 'IBM']\n",
    "    initial_cash = 10000\n",
    "    \n",
    "    # Load and prepare data\n",
    "    combined_data = pd.concat([load_data(ticker, start_date, end_date).assign(Ticker=ticker) \n",
    "                             for ticker in tickers])\n",
    "    \n",
    "    # # Initialize models and training components\n",
    "    # models = {}\n",
    "    # for ticker in tickers:\n",
    "    #     model = LSTMTrader(input_size=10, hidden_size=64, output_size=len(ACTION_SPACE))\n",
    "    #     optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    #     criterion = nn.MSELoss()\n",
    "    #     replay_buffer = ReplayBuffer(MEMORY_SIZE)\n",
    "        \n",
    "    #     # Train model\n",
    "    #     print(f\"\\nTraining model for {ticker}...\")\n",
    "    #     ticker_data = combined_data[combined_data['Ticker'] == ticker].copy()\n",
    "    #     models[ticker] = train_model(model, ticker, ticker_data, replay_buffer, optimizer, criterion)\n",
    "\n",
    "    #     # Save model, optimizer, and additional parameters after training\n",
    "    #     save_path = f\"{ticker}_model.pth\"\n",
    "    #     torch.save({\n",
    "    #         'model_state_dict': model.state_dict(),\n",
    "    #         'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #         'criterion': criterion,\n",
    "    #         'replay_buffer': replay_buffer,\n",
    "    #         'epsilon': EPSILON\n",
    "    #     }, save_path)\n",
    "    #     print(f\"Model, optimizer, and parameters for {ticker} saved to {save_path}\")\n",
    "        \n",
    "    # # Initialize ensemble trader and run simulation\n",
    "    # print(\"\\nRunning trading simulation...\")\n",
    "    # ensemble_trader = EnsembleTrader(models, combined_data)\n",
    "    # trades, portfolio_values, final_cash, final_shares = ensemble_trader.simulate_trading(initial_cash)\n",
    "    \n",
    "    # # Calculate and display metrics\n",
    "    # metrics = calculate_metrics(trades, portfolio_values, initial_cash)\n",
    "    \n",
    "    # print(\"\\nTrading Results:\")\n",
    "    # print(\"=\" * 50)\n",
    "    # print(f\"Final Balance: ${metrics['Final Balance']:.2f}\")\n",
    "    # print(f\"Total Returns: {metrics['Total Return']:.2f}%\")\n",
    "    # print(f\"Win Rate: {metrics['Win Rate']:.2f}\")\n",
    "    # print(f\"Volatility: {metrics['Volatility']:.4f}\")\n",
    "    # print(f\"Sharpe Ratio: {metrics['Sharpe Ratio']:.4f}\")\n",
    "    \n",
    "    # print(\"\\nFinal Positions:\")\n",
    "    # for ticker, shares in final_shares.items():\n",
    "    #     print(f\"{ticker}: {shares} shares\")\n",
    "    # print(f\"Cash: ${final_cash:.2f}\")\n",
    "    \n",
    "    # print(\"\\nTrading History (first 10 trades):\")\n",
    "    # for t in trades[:10]:\n",
    "    #     print(f\"Day {t[0]}: {t[1]} - {t[2]} {t[3]} shares at ${t[4]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Constants\n",
    "EPSILON = 0.1\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DECAY = 0.8\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "MEMORY_SIZE = 10000\n",
    "WINDOW_SIZE = 3\n",
    "ACTION_SPACE = np.array([-1.0, 0.0, 1.0])  # Sell, Hold, Buy\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state):\n",
    "        self.buffer.append((state, action, reward, next_state))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class LSTMTrader(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMTrader, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, num_layers=2, dropout=0.2)\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads=4)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        attn_output, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        final_output = attn_output[:, -1, :]\n",
    "        x = self.relu(self.fc1(final_output))\n",
    "        return self.fc2(x)\n",
    "\n",
    "def add_technical_indicators(df):\n",
    "    # Calculate moving averages\n",
    "    df['SMA_5'] = df['Close'].rolling(window=5).mean()\n",
    "    df['SMA_20'] = df['Close'].rolling(window=20).mean()\n",
    "    \n",
    "    # Calculate RSI\n",
    "    delta = df['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['RSI'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Calculate MACD\n",
    "    exp1 = df['Close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['Close'].ewm(span=26, adjust=False).mean()\n",
    "    df['MACD'] = exp1 - exp2\n",
    "    df['Signal_Line'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "    \n",
    "    # Calculate Bollinger Bands\n",
    "    df['BB_middle'] = df['Close'].rolling(window=20).mean()\n",
    "    df['BB_upper'] = df['BB_middle'] + 2 * df['Close'].rolling(window=20).std()\n",
    "    df['BB_lower'] = df['BB_middle'] - 2 * df['Close'].rolling(window=20).std()\n",
    "    \n",
    "    # Add volume indicators\n",
    "    df['Volume_SMA'] = df['Volume'].rolling(window=5).mean()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_state(df, current_idx, window_size):\n",
    "    if current_idx < window_size:\n",
    "        return None\n",
    "    \n",
    "    state = []\n",
    "    for i in range(current_idx - window_size, current_idx):\n",
    "        features = [\n",
    "            df['Close'].iloc[i],\n",
    "            df['SMA_5'].iloc[i],\n",
    "            df['SMA_20'].iloc[i],\n",
    "            df['RSI'].iloc[i],\n",
    "            df['MACD'].iloc[i],\n",
    "            df['Signal_Line'].iloc[i],\n",
    "            df['BB_upper'].iloc[i],\n",
    "            df['BB_lower'].iloc[i],\n",
    "            df['Volume_SMA'].iloc[i],\n",
    "            df['Returns'].iloc[i]\n",
    "        ]\n",
    "        state.append(features)\n",
    "    return np.array(state)\n",
    "\n",
    "def load_model(ticker, model_path):\n",
    "    \"\"\"Load a saved model for a specific ticker\"\"\"\n",
    "    model = LSTMTrader(input_size=10, hidden_size=64, output_size=len(ACTION_SPACE))\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    return model\n",
    "\n",
    "def load_data(ticker, start_date, end_date):\n",
    "    \"\"\"Load and prepare data for a specific ticker\"\"\"\n",
    "    df = yf.download(ticker, start=start_date, end=end_date)\n",
    "    df = add_technical_indicators(df)\n",
    "    df['Returns'] = df['Close'].pct_change()\n",
    "    return df.dropna()\n",
    "\n",
    "class TradingSimulator:\n",
    "    def __init__(self, models, commission=0.001):\n",
    "        self.models = models\n",
    "        self.commission = commission\n",
    "    \n",
    "    def get_action(self, state, ticker):\n",
    "        \"\"\"Get trading action from model\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = self.models[ticker](state_tensor)\n",
    "            return ACTION_SPACE[q_values.max(1)[1].item()]\n",
    "    \n",
    "    def calculate_portfolio_value(self, cash, positions, current_prices):\n",
    "        \"\"\"Calculate total portfolio value\"\"\"\n",
    "        value = cash\n",
    "        for ticker, shares in positions.items():\n",
    "            value += shares * current_prices[ticker]\n",
    "        return value\n",
    "    \n",
    "    def simulate(self, data_dict, initial_cash=10000, simulation_days=None):\n",
    "        \"\"\"\n",
    "        Simulate trading using loaded models\n",
    "        \n",
    "        Parameters:\n",
    "        data_dict: Dictionary of DataFrames for each ticker\n",
    "        initial_cash: Initial cash amount\n",
    "        simulation_days: Number of days to simulate (None for entire period)\n",
    "        \"\"\"\n",
    "        cash = initial_cash\n",
    "        positions = {ticker: 0 for ticker in self.models.keys()}\n",
    "        portfolio_values = []\n",
    "        trades = []\n",
    "        trade_history = []\n",
    "        \n",
    "        # Determine simulation period\n",
    "        min_length = min(len(df) for df in data_dict.values())\n",
    "        start_idx = WINDOW_SIZE\n",
    "        end_idx = min_length if simulation_days is None else min(start_idx + simulation_days, min_length)\n",
    "        \n",
    "        # Get common date range\n",
    "        dates = list(data_dict[list(data_dict.keys())[0]].index[start_idx:end_idx])\n",
    "        \n",
    "        for t, current_date in enumerate(dates, start=start_idx):\n",
    "            current_prices = {\n",
    "                ticker: data_dict[ticker]['Close'].loc[current_date]\n",
    "                for ticker in self.models.keys()\n",
    "            }\n",
    "            \n",
    "            # Store daily portfolio value\n",
    "            portfolio_value = self.calculate_portfolio_value(cash, positions, current_prices)\n",
    "            portfolio_values.append((current_date, portfolio_value))\n",
    "            \n",
    "            # Make trading decisions for each ticker\n",
    "            for ticker in self.models.keys():\n",
    "                ticker_data = data_dict[ticker]\n",
    "                state = prepare_state(ticker_data, t, WINDOW_SIZE)\n",
    "                \n",
    "                if state is not None:\n",
    "                    action = self.get_action(state, ticker)\n",
    "                    current_price = current_prices[ticker]\n",
    "                    \n",
    "                    # Execute trades based on action\n",
    "                    if action > 0:  # Buy\n",
    "                        max_shares = int(cash / (current_price * (1 + self.commission)))\n",
    "                        if max_shares > 0:\n",
    "                            cost = max_shares * current_price * (1 + self.commission)\n",
    "                            cash -= cost\n",
    "                            positions[ticker] += max_shares\n",
    "                            trades.append({\n",
    "                                'date': current_date,\n",
    "                                'ticker': ticker,\n",
    "                                'action': 'BUY',\n",
    "                                'shares': max_shares,\n",
    "                                'price': current_price,\n",
    "                                'cost': cost\n",
    "                            })\n",
    "                    \n",
    "                    elif action < 0:  # Sell\n",
    "                        if positions[ticker] > 0:\n",
    "                            shares_to_sell = positions[ticker]\n",
    "                            revenue = shares_to_sell * current_price * (1 - self.commission)\n",
    "                            cash += revenue\n",
    "                            positions[ticker] = 0\n",
    "                            trades.append({\n",
    "                                'date': current_date,\n",
    "                                'ticker': ticker,\n",
    "                                'action': 'SELL',\n",
    "                                'shares': shares_to_sell,\n",
    "                                'price': current_price,\n",
    "                                'revenue': revenue\n",
    "                            })\n",
    "            \n",
    "            # Record daily state\n",
    "            trade_history.append({\n",
    "                'date': current_date,\n",
    "                'portfolio_value': portfolio_value,\n",
    "                'cash': cash,\n",
    "                'positions': positions.copy()\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'trades': trades,\n",
    "            'portfolio_values': portfolio_values,\n",
    "            'final_cash': cash,\n",
    "            'final_positions': positions,\n",
    "            'trade_history': trade_history\n",
    "        }\n",
    "\n",
    "def calculate_metrics(simulation_result, initial_cash):\n",
    "    \"\"\"Calculate performance metrics from simulation results\"\"\"\n",
    "    portfolio_values = [pv[1] for pv in simulation_result['portfolio_values']]\n",
    "    returns = np.diff(portfolio_values) / portfolio_values[:-1]\n",
    "    \n",
    "    if len(portfolio_values) < 2:\n",
    "        return {}\n",
    "    \n",
    "    total_return = (portfolio_values[-1] - initial_cash) / initial_cash\n",
    "    daily_returns = pd.Series(returns)\n",
    "    \n",
    "    metrics = {\n",
    "        'Final Portfolio Value': portfolio_values[-1],\n",
    "        'Total Return (%)': total_return * 100,\n",
    "        'Number of Trades': len(simulation_result['trades']),\n",
    "        'Win Rate (%)': (daily_returns > 0).mean() * 100,\n",
    "        'Sharpe Ratio': np.sqrt(252) * daily_returns.mean() / daily_returns.std() if len(daily_returns) > 0 else 0,\n",
    "        'Max Drawdown (%)': (pd.Series(portfolio_values).diff() / pd.Series(portfolio_values).shift(1)).min() * 100,\n",
    "        'Final Cash': simulation_result['final_cash'],\n",
    "        'Volatility (%)': daily_returns.std() * np.sqrt(252) * 100\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    tickers = ['AAPL', 'IBM']  # Add more tickers as needed\n",
    "    start_date = '2023-01-01'  # Simulation start date\n",
    "    end_date = '2024-01-01'    # Simulation end date\n",
    "    initial_cash = 10000\n",
    "    simulation_days = 30  # Set to None for entire period\n",
    "    \n",
    "    # Load models\n",
    "    models = {}\n",
    "    for ticker in tickers:\n",
    "        model_path = f\"{ticker}_Ensemblemodel.pth\"\n",
    "        try:\n",
    "            models[ticker] = load_model(ticker, model_path)\n",
    "            print(f\"Loaded model for {ticker}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"No model found for {ticker}\")\n",
    "            continue\n",
    "    \n",
    "    # Load data\n",
    "    data_dict = {}\n",
    "    for ticker in models.keys():\n",
    "        data_dict[ticker] = load_data(ticker, start_date, end_date)\n",
    "        print(f\"Loaded data for {ticker}\")\n",
    "    \n",
    "    # Initialize simulator\n",
    "    simulator = TradingSimulator(models)\n",
    "    \n",
    "    # Run simulation\n",
    "    print(\"\\nRunning trading simulation...\")\n",
    "    simulation_result = simulator.simulate(\n",
    "        data_dict,\n",
    "        initial_cash=initial_cash,\n",
    "        simulation_days=simulation_days\n",
    "    )\n",
    "    \n",
    "    # Calculate and display metrics\n",
    "    metrics = calculate_metrics(simulation_result, initial_cash)\n",
    "    \n",
    "    print(\"\\nSimulation Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:,.2f}\")\n",
    "    \n",
    "    print(\"\\nFinal Positions:\")\n",
    "    for ticker, shares in simulation_result['final_positions'].items():\n",
    "        if shares > 0:\n",
    "            print(f\"{ticker}: {shares} shares\")\n",
    "    \n",
    "    print(\"\\nRecent Trades:\")\n",
    "    for trade in simulation_result['trades'][-5:]:  # Show last 5 trades\n",
    "        print(f\"Date: {trade['date'].strftime('%Y-%m-%d')}\")\n",
    "        print(f\"Ticker: {trade['ticker']}\")\n",
    "        print(f\"Action: {trade['action']}\")\n",
    "        print(f\"Shares: {trade['shares']}\")\n",
    "        print(f\"Price: ${trade['price']:.2f}\")\n",
    "        print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
